---
title: "The Central Limit Theorem As I Understand It"
date: '2019-05-19'
slug: central-limit-theorem
tags: ['math']
excerpt: "The Central Limit Theorem took a long time for me to understand it. What helped me was to take it in little by little."
meta: false
draft: false
comment: true
bibliography: references.bib
csl: apa.csl
output:
        blogdown::html_page:
                toc: true
                number_sections: true
---

According to @lady, the Central Limit Theorem (CLT) didn't have an explicit proof when it was first written down by Abraham de Moivre in the 18th century. For a long time, it was a folk thoerem, a conjecture that even eminent mathematicians like Arnorld Fisher assumed to be true until the early 20th century. That a keystone theorem to the theory of probability and statistics was merely a conjecture for a long time made me feel better about myself: I didn't understand the CLT when I first learned about it, and it wasn't until I saw it written in a different form that I finally found it tangible. In this blog post, I want to develop the theorem from its baby versions to its adult versions. Before ending the blog post, I share and summarize some articles that I found while looking up how to apply the CLT on non-iid samples. 

# The Central Limit Theorem for IID Samples

## The Fetus

I call this the 'fetus' version because I consider it too underdeveloped to properly be considered a 'baby' version of the CLT. Nonetheless, I find it worth being written down, for it gave me the grounds to believe in the more mature form of the theorem. 

> *Let there be two independent random variables, $X_1$ and $X_2$, where $X_i \sim N(\mu_i, \sigma^2_i)$ for $i = 1, 2$. The distribution of the sum is also normal: $$X_1 + X_2 \sim N(\mu_1 + \mu_2, \sigma^2_1 + \sigma_2^2).$$*


There are different ways to prove the statement, but I find the one using moment generating functions (mgf) to be the easiest. 

{{% aside %}}
**Proof**: Recall that for $X \sim N(\mu, \sigma^2)$, the mgf is $M_X(t) = \exp\big\{ t\mu + \dfrac{1}{2}\sigma^2 t^2\big\}$. We will use the property of mgf, which says that the mgf of independent sums of random variables is the product of their mgf's. 
$$
        \begin{align*}
        M_{X_1 + X_2}(t) & = M_{X_1}(t)M_{X_2}(t) \quad \text{by said property of mgf} \\
        & = \exp\Big\{ \mu_1 t + \dfrac{1}{2}\sigma_1^2 t^2 \Big\}\exp\Big\{ \mu_2 t + \dfrac{1}{2}\sigma_2^2 t^2 \Big\} \\
        & = \exp\Big\{ t(\mu_1 + \mu_2) + \dfrac{1}{2}(\sigma_1^2 + \sigma_2^2)t^2 \Big\} \\
        & = M_W(t) \qquad \forall t \in \mathbb{R}
        \end{align*}
$$
where $W \sim N(\mu_1 + \mu_2, \sigma^2_1 + \sigma^2_2)$. Thus, the sum of two independent normal r.v. is normally distributed. $\heartsuit$
{{% /aside %}}

I specifically stated the sum of *two* independent random variables, but note that the result holds for a sum of any $n \in \mathbb{N}$ independent normal r.v.'s as 


## The Baby CLT

The baby version extends the fetus version by considering the standardized sample mean of $n \in \mathbb{N}$ independent normal r.v.'s. 

> *Let $X_1, X_2, \ldots \overset{\rm{iid}}{\sim} N(\mu, \sigma^2) \; \forall n \in \mathbb{N}$. Let $\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^n X_i$ be the sample mean of $\{X_i\}_{i\leq n}$. Then standardizing $\bar{X}_n$ gives $$\bar{X}^{\rm{st}}_n = \dfrac{\bar{X}_n - \mu}{\sigma\sqrt{n}} \sim N(0, 1)$$ for all $n \in \mathbb{N}$.*

{{% aside %}}
**Proof**: Let's manipulate $\bar{X}_n^{\rm{st}}$ and see where it leads to.

$$
        \begin{align*}
        \bar{X}_n^{\rm{st}} & = \dfrac{\bar{X}_n - \mu}{\sqrt{\sigma^2/n}} \\
        & = \dfrac{\sqrt{n}}{\sigma}(\bar{X}_n - \mu) \\
        & = \dfrac{\sqrt{n}}{\sigma}\Big[ \Big( \dfrac{1}{n}\sum_{i=1}^n X_i \Big) - \mu \Big] \\
        & = \dfrac{\sqrt{n}}{\sigma}\Big[\Big( \dfrac{1}{n}\sum_{i=1}^n X_i \Big) - \dfrac{n\mu}{n} \Big] \\
        & = \dfrac{1}{\sigma \sqrt{n}}\Big[ \sum_{i=1}^n (X_i - \mu) \Big] \\
        & = \dfrac{1}{\sqrt{n}} \sum_{i=1}^n \dfrac{X_i - \mu}{\sigma} \\
        & = \dfrac{1}{\sqrt{n}} \sum_{i=1}^n Z_i
        \end{align*}
$$
Note that $Z_i = \dfrac{X_i - \mu}{\sigma} \overset{\rm{iid}}{\sim} N(0, 1)$; according to [fetus CLT](#fetus) summing these standard normal r.v.'s gives $S = \sum_{i=1}^n Z_i \overset{\rm{iid}}{\sim} N(0, n)$. Let's find the mean and variance of $\dfrac{S}{\sqrt{n}}$. 
$$
        \begin{align*}
        E \bigg[ \dfrac{1}{\sqrt{n}}S \bigg] & = \dfrac{1}{\sqrt{n}}E[S] = 0 \\
        \rm{Var} \bigg(\dfrac{1}{\sqrt{n}}S \bigg) & = \dfrac{1}{n}\rm{Var}(S) = \dfrac{n}{n} = 1
        \end{align*}
$$
We're getting closer to the finish line: the next important part of the proof is to show that $\dfrac{1}{\sqrt{n}}S$ follows a normal distribution. I can show this using mgf uniqueness. 



$$
        \begin{align*}
        M_{\frac{S}{\sqrt{n}}}(t) & = E \bigg[\exp\bigg\{ \dfrac{S}{\sqrt{n}} t \bigg\} \bigg] \\
        & = E \bigg[ \exp\bigg\{S \Big(\dfrac{t}{\sqrt{n}}\Big)  \bigg\} \bigg] \\
        & = \exp \bigg\{ \dfrac{t}{\sqrt{n}}(0) + \dfrac{1}{2}\Big( \dfrac{t}{\sqrt{n}} \Big)^2 n \bigg\} \quad \text{because $S \sim N(0, n)$}\\
        & = \exp \Big\{ \dfrac{t^2}{2} \Big\} \\
        & = M_Z(t) \quad \text{where $Z \sim N(0,1)$}
        \end{align*}
$$
Therefore, by mgf uniqueness, $\bar{X}_n^{\rm{st}} = \dfrac{S}{\sqrt{n}} \sim N(0, 1)$. $\heartsuit$

{{% /aside %}}

## The Adult CLT, v1.0

The baby CLT assumed that $X_1, X_2, \ldots \overset{\rm{iid}}{\sim} N(\mu, \sigma^2)$. The classical (i.e. adult) form of CLT assumes only that $X_1, X_2, \ldots$ be independent and identically distributed and that $n$ be large enough. 

> *Let $X_1, X_2, \ldots \sim F_X$ where $E[X] = \mu$ and $0 < \rm{Var}(X) = \sigma^2 < \infty$. Let $\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^n X_i$. Then, $$\bar{X}_n^{\rm{st}} = \dfrac{\bar{X}_n - \mu}{\sigma/\sqrt{n}} \xrightarrow{\mathcal{D}} N(0, 1).$$*

{{% aside %}}
**Proof**: The main tool for this proof is the mgf convergence theorem (aka LÃ©vy continuity theorem). Suppose there exists $\delta$ such that $\forall t \in (-\delta, \delta)$, $$\lim_{n \to \infty} M_{X_n}(t) = M_X(t).$$ Then the mgf convergence theorem says that $X_n \xrightarrow{\mathcal{D}} X$. 

:thought_bubble: Proving the mgf convergence is beyond my expertise as it involves real analysis. I hope I can get there some day though. 


The first half of the proof is the same as [baby CLT](#baby), i.e. we do the same algebraic manipulation to get $$\bar{X}_n^{\rm{st}} = \dfrac{1}{\sqrt{n}} \sum_{i=1}^n Z_i$$ where $Z_i = \dfrac{X_i - \mu}{\sigma} \sim N(0, 1)$. Now let's look at the mgf of $\bar{X}_n^{\rm{st}}$.
$$
        \begin{align*}
        M_{\bar{X}_n^{\rm{st}}}(t) & = E \Big[\exp\Big\{\bar{X}_n^{\rm{st}}t \Big\} \Big]\\
        & = E \Big[ \exp \Big\{ \dfrac{t}{\sqrt{n}}\sum_{i=1}^n Z_i \Big\} \Big] \\
        & = M_{\sum_i Z_i}\Big( \dfrac{t}{\sqrt{n}} \Big) \\
        & = \bigg[ M_{Z_i} \Big( \dfrac{t}{\sqrt{n}} \Big) \bigg]^n \\
        & = \bigg[ g\Big( \dfrac{t}{\sqrt{n}} \Big) \bigg]^n \quad \text{where $g(t) = M_{Z}(t)$} \\
        &= \clubsuit
        \end{align*}
$$

The mgf convergence theorem tells us to take the last expression to infinity, but notice what happens to the last expression as $n \rightarrow \infty$. Since $\dfrac{t}{\sqrt{n}} \rightarrow 0$, 
$$ \lim_{n \to \infty} g\bigg(\dfrac{t}{\sqrt{n}}\bigg)  = g(0) = a$$ for some constant $a$. But $a^n \rightarrow \infty$ as $n \rightarrow \infty$. 

Here, I will use Taylor series expansion on $g$. Recall that by [Taylor series expansion](`r blogdown::shortcode('ref', '/posts/2019-05-08-taylor-series')`), the Taylor series expansion of $g(s)$ at $s=0$ is
$$
        \begin{align*}
        g(s) & = g^0(0) + g^\prime(0)(s) + \dfrac{g^{\prime\prime}(0)}{2!}(s^2) + \cdots \\
        & \approx  1 + g^\prime(0)(s) + g^{\prime\prime}(0)\dfrac{s^2}{2} \quad \text{when $s$ is close to 0} \\
        & \approx 1 + 0(s) + (1)\dfrac{s^2}{2} \quad \text{b/c $g$ is mgf of standard normal} \\
        & = 1 + \dfrac{s^2}{2}
        \end{align*}
$$
The assumption that $g(\cdot)$ will be evaluated close to 0 is already known because we know that $\dfrac{t}{\sqrt{n}} \rightarrow 0$ as $n \rightarrow \infty$. Let's use this approximation of $g(s)$ and continue the derivation after $\clubsuit$:
$$
        \begin{align*}
        M_{\bar{X}_n^{\rm{st}}}(t) & = \clubsuit \\
        & = \bigg[ 1 + \dfrac{(t/\sqrt{n})^2}{2} \bigg]^n  \\
        & = \bigg[ 1 + \dfrac{t^2/2}{n} \bigg]^n 
        \end{align*}
$$
Note that this resembles a common limit: $\underset{n \to \infty}{\lim} \Big(1 + \dfrac{c_n}{n} \Big) = e^c$ assuming that $c_n \rightarrow n$ as $n \to \infty$. I can take the limit of this mgf to see that it converges to the mgf of standard normal distribution, $Z \sim N(0, 1)$.
$$
        \begin{align*}
        \lim_{n \to \infty}M_{\bar{X}_n^{\rm{st}}}(t) = \lim_{n \to \infty}\bigg[ 1 + \dfrac{t^2/2}{n} \bigg]^n  = \exp\Big\{ \dfrac{t^2}{2} \Big\} = M_Z(t) 
        \end{align*}
$$

Thus, for all $t \in \mathbb{R}$, $M_{\bar{X}_n^{\rm{st}}} \xrightarrow{\mathcal{D}} M_Z(t).$ By mgf convergence, we conclude that $$ \bar{X}_n^{\rm{st}} \underset{n \to \infty}{\rightarrow} Z \sim N(0, 1). \, \heartsuit$$

## The Adult CLT, v2.0

Even at this point I felt fuzzy about the CLT. Specifically, I wasn't sure how to use the theorem ... until I saw it recast in a different form.

Recall that the CLT says $$\dfrac{\bar{X}_n^{\rm{st}} - \mu}{\sigma/\sqrt{n}} \; \overset{\mathcal{D}}{\rightarrow} \; N(0, 1)$$ where $\mu = E[X]$ and $0 < \sigma^2 = \rm{Var}(X) < \infty$. In other words, when $n$ is large enough, 
$$
        \begin{align*}
        \dfrac{\bar{X}_n^{\rm{st}} - \mu}{\sigma/\sqrt{n}} &\approx Z \\
        \sqrt{n}\big(\bar{X}_n^{\rm{st}} - \mu \big) &\approx Z \sigma \\
        \sqrt{n}\big(\bar{X}_n^{\rm{st}} - \mu \big) \;  &\overset{\mathcal{D}}{\rightarrow} \; N(0, \sigma^2) \quad \text{b/c $\rm{Var}(\sigma Z) = \sigma^2 (1) = \sigma^2$}
        \end{align*}
$$
Therefore, the CLT can be recast as 
        $$\sqrt{n}\big( \bar{X}_n^{\rm{st}} - E[X] \big) \; \overset{\mathcal{D}}{\rightarrow} \; N\big(0, \rm{Var}(X) \big),$$
which says that when you average a sample of iid random variables, standardize it, take its difference from the population mean, and magnify the difference by $\sqrt{n}$, you get a normal distribution with mean 0 and variance equal to the population variance.
{{% /aside %}}



:thought_balloon: This is my favorite way of writing the CLT because a.) no ugly fractions, and b.) it becomes easier to transition to concepts like reparametrization delta method and asymptotic variance. Pedagogically, I was surprised that a theorem I had written down many times finally clicked with me when it was written differently.


# The Central Limit Theorem for Non-IID Samples 

We can deviate from the classical CLT setting either by a.) having independent but heterogeneous sample (e.g. the Lyapunov CLT) or b.) by having a dependent sample. Either way, any generalization of the CLT requires clearly demarcating, in the first case, the boundedness of the moments or, in the second case, the structure of dependence between the units. But if we know we are working with non-iid samples, how would we know that the CLT is applicable? @just1999crop and @koundouri2011crop ask this question with respect to the population distribution of agricultural crop yields.


## Are Agricultural Crop Yields Normally Distributed?


@just1999crop and @koundouri2011crop, both published in *American Journal of Agricultural Economics*, assess whether agricultural crop yields are normally distributed. On the one hand, despite the spatial and temporal correlation between crop yield measurements, @just1999crop speculates that the population distribtuion of agricultural crop yields is ultimately normal because "crop yields at all levels are averages ... and [the CLT] implies that averages have asymptotically normal distributions under broad conditions" (301). The paper argues that the existing empirical results claiming otherwise suffer from methodological flaws such as misspecification of the deterministic component of the measurements and erroneous significance tests that fail to adjust for multiple testing.

On the other hand, @koundouri2011crop argues that even after fixing these methodological issues, the limiting distribution of the detrended and demeaned crop yield measures may not be normal afterall. Focusing on the variability of the number of farms assigned to the production of a type of crop in a given year, the authors argue that the problem is better framed as one of finding the limiting distribution of random sums of random variables (7). The authors conduct their empirical analysis in accordance with this interpretation and conclude that year-to-year change in the total number of acreage dedicated to one crop are associated with nonzero skewness and kurtosis.

*n.b.*: The skewness and kurtosis of a normal distribution are zero.



:thought_balloon: Personally, I was intrigued by how one would go about assessing normality with real data. I cherry-picked the two articles because they were both published in the same journal, but there are many other articles that tackle the same descriptive question---with @koundouri2011crop being the most recent contribution to the discourse---and others that ask related and equally interesting questions. For example, @history2019crop asks whether crop yields measured over the years come from identical distribution and warns against including every historical observation in the sample just to up the sample size. I appreciate articles like these---they ask basic descriptive question about the data at hand, the answers to which have nontrivial implications on how the data is to be used in further statistical inference. 


# References







