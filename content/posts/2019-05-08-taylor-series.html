---
title: "Taylor Series in Probability"
date: '2019-05-08'
slug: taylor-series
excerpt: "My grasp of Taylor series has been tenuous. I write this blog post to strengthen my understanding by laying out some of the ways Taylor series is invoked in probability."
tags: ['math']
meta: true
comment: true
draft: false
output:
        blogdown::html_page:
                toc: true
                number_sections: true
---


<div id="TOC">
<ul>
<li><a href="#definition"><span class="toc-section-number">1</span> Definition</a><ul>
<li><a href="#taylor-series-of-common-functions"><span class="toc-section-number">1.1</span> Taylor Series of Common Functions</a></li>
</ul></li>
<li><a href="#interesting-properties"><span class="toc-section-number">2</span> Interesting Properties</a><ul>
<li><a href="#taylor-series-uniqueness"><span class="toc-section-number">2.1</span> Taylor Series Uniqueness</a></li>
<li><a href="#taylors-theorem"><span class="toc-section-number">2.2</span> Taylor’s Theorem</a></li>
</ul></li>
<li><a href="#application"><span class="toc-section-number">3</span> Application</a><ul>
<li><a href="#moment-generating-property"><span class="toc-section-number">3.1</span> Moment Generating Property</a></li>
<li><a href="#mgf-of-poisson-distribution"><span class="toc-section-number">3.2</span> Mgf of Poisson Distribution</a></li>
<li><a href="#moment-list-of-standard-normal-dist."><span class="toc-section-number">3.3</span> Moment List of Standard Normal Dist.</a></li>
</ul></li>
</ul>
</div>

<p>I first learned about Taylor series in high school, but I didn’t quite master series expansion in general, let alone Taylor. Even after several math courses in college, my familiarity with it has barely grown since our first encounter back in high school. I find this state of affairs embarrassing, so I want to understand a little bit more about Taylor series by writing a blog post. This blog post will begin with the definition of Taylor series and its properties and end with some examples of application of Taylor series in probability.</p>
<div id="definition" class="section level1">
<h1><span class="header-section-number">1</span> Definition</h1>
<p>Taylor Series belongs to a type of series expansion of a function <span class="math inline">\(f(x)\)</span> known as power series whose general form is
<span class="math display">\[ \sum_{n=0}^\infty a_n (x-c)^n\]</span>
for some constant <span class="math inline">\(c\)</span> and coefficients <span class="math inline">\(\{a_n\}\)</span> independent of <span class="math inline">\(x\)</span>. A Taylor Series of a real (or complex), infinitely differentiable function <span class="math inline">\(f(x)\)</span> at a point <span class="math inline">\(a\)</span> is given by <span class="math display">\[ f(x) = f(a) + f^\prime(a)(x - a) + \dfrac{f^{\prime \prime}(a)}{2!}(x-a)^2 + \dfrac{f^{(3)}(a)}{3!}(x-a)^3 + \cdots .\]</span></p>
<p>For example, let <span class="math inline">\(f(x) = \dfrac{1}{\sqrt{2\pi}}\exp \Big\{ \dfrac{x^2}{2} \Big\}\)</span>, the pdf of standard normal distribution. We want to find the Taylor series of <span class="math inline">\(f(x)\)</span> at <span class="math inline">\(x = 0\)</span>. For calculation purposes, I am only going to do up to the 5th term.
<span class="math display">\[
        \begin{align*}
        f(0) &amp; = \dfrac{1}{\sqrt{2\pi}} \\
        f^\prime(0) &amp; = \dfrac{1}{\sqrt{2\pi}}\exp \Big\{ \dfrac{x^2}{2} \Big\}x\Big|_{x=0} = 0 \\
        f^{\prime \prime}(0) &amp; = \dfrac{1}{\sqrt{2\pi}}\exp \Big\{ \dfrac{x^2}{2} \Big\} + \dfrac{1}{\sqrt{2\pi}}\exp \Big\{ \dfrac{x^2}{2} \Big\}x^2 \Big|_{x=0} = \dfrac{1}{\sqrt{2\pi}} \\
        f^{(3)}(0) &amp; = \dfrac{1}{\sqrt{2\pi}}\exp \Big\{ \dfrac{x^2}{2} \Big\}x + \dfrac{1}{\sqrt{2\pi}}\exp \Big\{ \dfrac{x^2}{2} \Big\} (2x) + \dfrac{1}{\sqrt{2\pi}}\exp\Big\{ \dfrac{x^2}{2} \Big\}x^3 \Big|_{x=0} \\
        &amp; = \dfrac{3}{\sqrt{2\pi}}\exp \Big\{ \dfrac{x^2}{2} \Big\}x + \dfrac{1}{\sqrt{2\pi}} \exp \Big\{ \dfrac{x^2}{2}\Big\}x^3 \Big|_{x=0} = 0\\
        f^{(4)}(0) &amp; = \dfrac{3}{\sqrt{2\pi}} \exp \Big\{ \dfrac{x^2}{2} \Big\} + \dfrac{6}{\sqrt{2\pi}}\exp\Big\{ \dfrac{x^2}{2} \Big\}x^2 + \dfrac{1}{\sqrt{2\pi}}\exp\Big\{\dfrac{x^2}{2}\Big\}x^4 \Big|_{x=0} = \dfrac{3}{\sqrt{2\pi}} \\
        f^{(5)}(0) &amp; = 0
        \end{align*}
\]</span>
Putting it all together, we have the Taylor series for the pdf of standard normal distribution:
<span class="math display">\[ f(x) = \dfrac{1}{\sqrt{2\pi}} + \dfrac{1}{2! \sqrt{2\pi}}x^2 + \dfrac{3}{4!\sqrt{2\pi}}x^4 + O(x^6)\]</span></p>
<p>For me, Kalid Azad at <em>Better Explained</em> gives the best interpretation of Taylor series. In the article, <a href="https://betterexplained.com/articles/taylor-series/">“Intuition for Taylor Series (DNA Analogy),”</a> Kalid describes a Taylor series of a function as “[a string of DNA] pulled from a single point to rebuild the entire function.” In the above example, the coefficients <span class="math inline">\(\dfrac{1}{\sqrt{2\pi}}, 0, \dfrac{1}{2!\sqrt{2\pi}}, 0, \dfrac{3}{4!\sqrt{2\pi}}, \ldots\)</span> are the pieces of DNA strung together with polynomial terms to recreate (i.e. approximate) the pdf of standard normal distribution at <span class="math inline">\(x=0\)</span>.</p>
<p>Note that the coefficients would be different if we chose to center the Taylor series at a different point of <span class="math inline">\(a\)</span>. In other words, the Taylor series approximation is local and gets worse as we move away from that <span class="math inline">\(a\)</span> we chose.</p>
<div id="taylor-series-of-common-functions" class="section level2">
<h2><span class="header-section-number">1.1</span> Taylor Series of Common Functions</h2>
<p>Here are the Taylor series evaluated at <span class="math inline">\(0\)</span> of some common functions.</p>
<p><span class="math display">\[e^{x} = 1 + x + \dfrac{1}{2!}x^2 + \dfrac{1}{3!}x^3 + \cdots = \sum_{n=0}^\infty \dfrac{x^n}{n!}\]</span>
As we will see in <a href="#application">Application</a>, this is the most commonly used series expansion.</p>
<p><span class="math display">\[(1+x)^n = 1 + nx + \dfrac{n(n-1)}{2!}x^2 + \dfrac{n(n-1)(n-2)}{3!}x^3 + \cdots = \sum_{k=0}^n \binom{n}{k}x^k\]</span>
The second one can also be derived as a particular case of binomial theorem <span class="math inline">\((a+b)^n = \sum_{k=0}^n \binom{n}{k}a^k b^{n-k}\)</span> where <span class="math inline">\(b = 1\)</span>.</p>
</div>
</div>
<div id="interesting-properties" class="section level1">
<h1><span class="header-section-number">2</span> Interesting Properties</h1>
<div id="taylor-series-uniqueness" class="section level2">
<h2><span class="header-section-number">2.1</span> Taylor Series Uniqueness</h2>
<blockquote>
<p><em>If for some <span class="math inline">\(r &gt; 0\)</span> a power series <span class="math inline">\(\sum_{n=0}^\infty a_n(x - x_0)^n\)</span> converges to <span class="math inline">\(fxz)\)</span> for all <span class="math inline">\(\left|x - x_0\right| &lt; r\)</span>, then this series is the Taylor series for <span class="math inline">\(f\)</span> about the point <span class="math inline">\(z_0\)</span>.</em></p>
</blockquote>
<p>This uniqueness property builds upon the power series uniqueness, which states that if there are two converging power series, <span class="math inline">\(\sum_{n=0}^\infty c_n (x- x_0)^n\)</span> and <span class="math inline">\(\sum_{n=0}^\infty d_n (x- x_0)^n\)</span>, such that
<span class="math display">\[ \sum_{n=0}^\infty c_n (x- x_0)^n = \sum_{n=0}^\infty d_n (x - x_0)^n\]</span>
for all open intervals containin <span class="math inline">\(z_0\)</span>, then <span class="math inline">\(c_n = d_n\)</span> for all <span class="math inline">\(n \geq 0\)</span>. This motivates me to restate the Taylor series uniqueness:</p>
<blockquote>
<p><em>Suppose there exists <span class="math inline">\(r &gt; 0\)</span> such that <span class="math inline">\(f\)</span> can be represented as <span class="math display">\[f(x) = x_0 + c_1(x- x_0) + c_2 (x - x_0)^2 + c_3 (x - x_0)^3 + \cdots\]</span> for all <span class="math inline">\(\left| x - x_0 \right| &lt; r\)</span>. Then, <span class="math display">\[c_n = \dfrac{f^{(n)}(x_0)}{n!}.\]</span></em></p>
</blockquote>
</div>
<div id="taylors-theorem" class="section level2">
<h2><span class="header-section-number">2.2</span> Taylor’s Theorem</h2>
<p>The Taylor’s theorem is commonly stated in the following form:</p>
<blockquote>
<p><em>Let <span class="math inline">\(k \in \mathbb{N}_0\)</span>. Suppose <span class="math inline">\(f\)</span> has <span class="math inline">\(k+1\)</span> continuous derivatives on an open interval containing <span class="math inline">\(x_0\)</span>. Then for each <span class="math inline">\(z\)</span> in the interval, <span class="math display">\[ f(x) = \Big[ \sum_{n=0}^k \dfrac{f^{(k)}(x_0)}{k!}(x-x_0)^k \Big] + R_{k+1}(x)\]</span> where <span class="math inline">\(R_{k+1}(x) = \int_{x_0}^x \dfrac{f^{(k+1)}(t)}{k!}(x - t)^{k} \, dt.\)</span></em></p>
</blockquote>
<p>I had a hard time understanding how Taylor’s theorem is different from the definition of Taylor’s series, but what I understand to be the difference is that Taylor’s theorem gives the error term for when we use Taylor series to approximate <span class="math inline">\(f(x)\)</span>. In the following proof I will show that if we approximate a <span class="math inline">\(k+1\)</span> times differentiable function <span class="math inline">\(f\)</span> with Taylor series up to the <span class="math inline">\(k\)</span>th term, the approximation error is <span class="math inline">\(\int_{x_0}^x \dfrac{f^{(k+1)}(t)}{k!}(x-t)^k \, dt\)</span>.</p>
<p><strong>Proof</strong>: I will fix <span class="math inline">\(x\)</span> and <span class="math inline">\(x_0\)</span>. By the fundamental theorem of calculus,
<span class="math display">\[ f(x) = f(x_0) + \int_{x_0}^x f^\prime(t) \, dt.\]</span>
For integration by parts, I set <span class="math inline">\(u = f^\prime(t)\)</span> and <span class="math inline">\(dv = dt\)</span>.
<span class="math display">\[
        \begin{align*}
        u = f^\prime(t) &amp;\quad dv = dt \\
        du = f^{\prime \prime}(t) \, dt &amp; \quad v = t - x
        \end{align*}
        \]</span>
Then, I get
<span class="math display">\[
\begin{align*}
        f(x) &amp; = f(x_0) + f^\prime(t)(t- x)\bigg|_{t=x_0}^{t=x} - \int_{x_0}^x f^{\prime \prime}(t)(t- x)\, dt \\
        &amp; = f(x_0) - f^\prime(x_0)(x_0 - x) - \int_{x_0}^x f^{\prime \prime}(t)(t- x)\, dt \\
        &amp; = f(x_0) + f^\prime(x_0)(x - x_0) - \int_{x_0}^x f^{\prime \prime}(t)(t- x)\, dt 
\end{align*}
\]</span></p>
<p>I repeat integration by parts until the <span class="math inline">\(k\)</span>th term.
<span class="math display">\[
\begin{align*}
        f(x) &amp; = f(x_0) + f^\prime(x_0)(x - x_0) - \int_{x_0}^x f^{\prime \prime}(t)(t- x_0)\, dt \\ 
        &amp; = f(x_0) + f^\prime(x_0)(x - x_0) - f^{\prime \prime}(t)\dfrac{(t-x_0)^2}{2}\bigg|_{t = x_0}^{t = x} + \int_{x_0}^x f^{(3)}(t)\dfrac{(t-x)^2}{2}\, dt \\
        &amp; = f(x_0) + f^\prime(x_0)(x- x_0) + f^{\prime\prime}(x_0)\dfrac{(x-x_0)^2}{2} + f^{(3)}(t)\dfrac{(t-x)^3}{3!}\bigg|_{t=x_0}^{t=x} - \int_{x_0}^x f^{(4)}(t)\dfrac{(t-x)^3}{3!}\, dt \\
        &amp; = f(x_0) + f^\prime(x_0)(x- x_0) + f^{\prime\prime}(x_0)\dfrac{(x-x_0)^2}{2} + f^{(3)}(x_0)\dfrac{(x-x_0)^3}{3!} + \cdots \\
        &amp;\qquad + f^{(k)}(x_0)\dfrac{(x-x_0)^k}{k!} + \int_{x_0}^x f^{(k+1)}(t)\dfrac{(t-x)^k}{k!}\, dt  \qquad \heartsuit.
\end{align*}
\]</span></p>
</div>
</div>
<div id="application" class="section level1">
<h1><span class="header-section-number">3</span> Application</h1>
<p>Taylor series comes in hand when proving the Central Limit Theorem and the delta method lemma, but I will reserve these for later posts. Here, I want to share other instances in probability where Taylor series is employed.</p>
<div id="moment-generating-property" class="section level2">
<h2><span class="header-section-number">3.1</span> Moment Generating Property</h2>
<p>Let <span class="math inline">\(m_k = E[X^k]\)</span> for <span class="math inline">\(k \in \mathbb{N_0}\)</span> be the <span class="math inline">\(k\)</span>th moment of random variable <span class="math inline">\(X\)</span>. Let <span class="math inline">\(M_X(t) = E[e^{tX}]\)</span> for all <span class="math inline">\(t \in \mathbb{R}\)</span> denote the moment generating function of <span class="math inline">\(X\)</span>. The moment generating property says that <span class="math inline">\(m_k = M_{X}^{(k)}(0)\)</span> if the moment generating function <span class="math inline">\(M_X(t)\)</span> is <strong>nice</strong> where a function <span class="math inline">\(g\)</span> is <strong>nice</strong> if <span class="math inline">\(g(t) = \sum_{k=0}^\infty \dfrac{g^{(k)}(0)}{k!}t^k\)</span> for all <span class="math inline">\(k \in \mathbb{N}_0\)</span>.</p>
<p><strong>Proof</strong>: Assume that a random variable <span class="math inline">\(X\)</span> has a nice mgf <span class="math inline">\(M_X(t)\)</span>.
<span class="math display">\[
\begin{align*}
        M_X(t) &amp; = E[e^{tX}] \qquad \text{by def. of mgf} \\
        &amp; = E \Big[ \sum_{k=0}^\infty \dfrac{(tX)^k}{k!} \Big] \qquad \text{by Taylor series expansion of $e^x$}\\
        &amp; = \sum_{k=0}^\infty \dfrac{t^kE[X^k]}{k!} \qquad \text{by linearity of expectation} \\
        &amp; = \sum_{k=0}^\infty \dfrac{t^k m_k}{k!} \qquad \text{by def. of moments}
\end{align*}
\]</span></p>
<p>Thus, we arrive at <span class="math inline">\(M_X(t) = \sum_{k=0}^\infty \dfrac{t^k m_k}{k!}\)</span>. We also know that <span class="math inline">\(M_X(t) = \sum_{k=0}^\infty \dfrac{M_X^{(k)}(0)}{k!}\)</span> from the following expansion:
<span class="math display">\[
\begin{align*}
        M_X(t)&amp; = E[e^{tX}] \qquad \text{by def. of mgf} \\
        &amp; = \int_{-\infty}^\infty e^{tx} f(x) \, dx \\
        &amp; = \int_{-\infty}^\infty (1 + tx + \dfrac{(tx)^2}{2!} + \cdots)f(x) \, dx \\
        &amp; = \int_{-\infty}^\infty f(x) \, dx + t\int_{-\infty}^\infty xf(x)\, dx + \dfrac{t^2}{2!}\int_{-\infty}^\infty x^2f(x)\, dx \\ 
        &amp; = 1 + tM^\prime_X(0) + \dfrac{t^2}{2!}M^{\prime \prime}_X(0) + \cdots \\
        &amp; = \sum_{k=0}^\infty \dfrac{M^{(k)}_X(0)}{k!}t^k
\end{align*}
\]</span>
By <a href="#taylor-series-uniqueness">Taylor series uniqueness</a>, the coefficiencts to the <span class="math inline">\(t^k\)</span> term must be equivalent for all <span class="math inline">\(k \in \mathbb{N}_0\)</span>, so
<span class="math display">\[
\begin{align*}
        \dfrac{m_k}{k!} &amp;= \dfrac{M^{(k)}_X(0)}{k!} \\
        m_k &amp; = M_X^{(k)}(0) \qquad \heartsuit.
\end{align*}
\]</span></p>
</div>
<div id="mgf-of-poisson-distribution" class="section level2">
<h2><span class="header-section-number">3.2</span> Mgf of Poisson Distribution</h2>
<p>Let <span class="math inline">\(X \sim \rm{Poisson}(\lambda)\)</span>. I want to derive the mgf of <span class="math inline">\(X\)</span>.
<span class="math display">\[
\begin{align*}
        M_X(t) &amp; = E[e^{tX}] \qquad \text{by def.} \\
        &amp; = \sum_{k=0}^\infty e^{tk} \dfrac{e^{-\lambda}\lambda^k}{k!} \qquad \text{by def. of expectation} \\
        &amp; = e^{-\lambda}\sum_{k=0}^\infty \dfrac{(e^t\lambda)^k}{k!} \\
        &amp; = e^{-\lambda}e^{e^t \lambda} \qquad \text{by exponential series} \\
        &amp; = e^{\lambda(e^t-1)} 
\end{align*}
\]</span>
Thus, <span class="math inline">\(M_X(t) = e^{\lambda(e^t - 1)}\)</span> for all <span class="math inline">\(t \in \mathbb{R}\)</span>. <span class="math inline">\(\heartsuit\)</span></p>
</div>
<div id="moment-list-of-standard-normal-dist." class="section level2">
<h2><span class="header-section-number">3.3</span> Moment List of Standard Normal Dist.</h2>
<p>Let <span class="math inline">\(Z \sim N(0, 1)\)</span>. Recall that the pdf of standard normal distribution is
<span class="math display">\[\phi(z) = \dfrac{1}{\sqrt{2\pi}} \exp \Big\{- \dfrac{z^2}{2} \Big\}.\]</span>
I want to find the moment list of <span class="math inline">\(Z\)</span>, <span class="math inline">\(\mathcal{M}_Z = \{ m_0, m_1, m_2, \ldots \} = \{m_k: k \in \mathbb{N}_0 \}\)</span>. We already have the first three moments:
<span class="math display">\[m_0 = 1, \, m_1 = 0, \, m_2 = 1.\]</span>
We know from <a href="#moment-generating-property">moment generating property</a> that we can extract <span class="math inline">\(m_k\)</span>’s from <span class="math inline">\(M_X^{(k)}(0)\)</span>’s. First, let’s derive the moment generating function of standard normal distribution, <span class="math inline">\(M_Z(t)\)</span>.
<span class="math display">\[
\begin{align*}
M_Z(t) &amp; = E[e^{tZ}] \\
        &amp; = \int_{-\infty}^\infty e^{tz}\phi(z) \, dz \\
        &amp; = \int_{-\infty}^\infty e^{tz} \dfrac{1}{\sqrt{2\pi}}\exp \Big\{ -\dfrac{z^2}{2} \Big\} \, dz \\
        &amp; = \int_{-\infty}^\infty \dfrac{1}{\sqrt{2\pi}}\exp \Big\{- \dfrac{1}{2}(z^2 + 2tz + t^2 - t^2) \Big\} \, dz \qquad \text{complete the squares} \\
        &amp; = \exp\Big\{ \dfrac{t^2}{2} \Big\} \int_{-\infty}^\infty \dfrac{1}{\sqrt{2\pi}}\exp \Big\{- \dfrac{1}{2}(z + t)^2  \Big\} \, dz \\
        &amp; = \exp\Big\{\dfrac{t^2}{2} \Big\} \int_{-\infty}^\infty \dfrac{1}{\sqrt{2\pi}}\exp\Big\{ - \dfrac{u^2}{2} \Big\} \, du \quad \text{by chain rule} \\
        &amp; = \exp\Big\{ \dfrac{t^2}{2} \Big\}\int_{-\infty}^\infty \phi(u) \, du \quad \text{where $\phi(u)$ is pdf of $N(0, 1)$} \\
        &amp; = \exp\Big\{ \dfrac{t^2}{2} \Big\} 
\end{align*}
\]</span>
By exponential series,
<span class="math display">\[
M_Z(t) = \exp\Big\{ \dfrac{t^2}{2} \Big\} = \sum_{k=0}^\infty \dfrac{(t^2/2)^k}{k!} = \sum_{k=0}^\infty \dfrac{t^{2k}}{2^k k!} 
\]</span>
So, how do we take this to express <span class="math inline">\(\mathcal{M}_Z\)</span>? Let’s expand the series out.
<span class="math display">\[
\begin{align*}
       M_Z(t) = \sum_{k=0}^\infty \dfrac{t^{2k}}{2^k k!}  &amp; = t^0 + \dfrac{t^2}{2} + \dfrac{t^4}{(2^2)2!} + \dfrac{t^6}{(2^3)3!} + \cdots \\
       &amp; = t^0 + 0t^1 + c_2t^2 + 0t^3 + c_4t^4 + \cdots
\end{align*}
\]</span>
For any random variable <span class="math inline">\(X\)</span>, the moment generating function has the following series expansion:
<span class="math display">\[M_X(t) = \sum_{n=0}^\infty \dfrac{m_n}{n!}t^n.\]</span>
So, according to the expansion of <span class="math inline">\(M_Z(t)\)</span>, we have the following for the first five moments for <span class="math inline">\(n \in \mathbb{N_0\)</span>:
<span class="math display">\[
        \begin{align*}
        \dfrac{m_0}{0!} = 1 \qquad \Rightarrow \qquad m_0 = 1 \\
        \dfrac{m_1}{1!} = 0 \qquad \Rightarrow \qquad m_1 = 0 \\
        \dfrac{m_2}{2!} = c_2 = \dfrac{1}{2^1 1!} \qquad \Rightarrow \qquad m_2 = 1 \\
        \dfrac{m_3}{3!} = 0 \qquad \Rightarrow \qquad m_3 = 0 \\ 
        \dfrac{m_4}{4!} = c_4 = \dfrac{1}{(2^2)2!} \qquad \Rightarrow \qquad m_4 = 3
        \end{align*}
\]</span></p>
<p>It’s clear that we have two cases of <span class="math inline">\(n\)</span>: when <span class="math inline">\(n\)</span> is odd and when <span class="math inline">\(n\)</span> is even.</p>
<ul>
<li><span class="math inline">\(n\)</span> is odd: <span class="math inline">\(m_n = 0\)</span>.</li>
<li><span class="math inline">\(n\)</span> is even, i.e. <span class="math inline">\(n = 2k\)</span> for some <span class="math inline">\(k \in \mathbb{N}_0\)</span>:
<span class="math display">\[
      \begin{align*}
      \dfrac{m_n}{n!} &amp;= \dfrac{1}{2^k k!} \\
      \dfrac{m_n}{(2k)!} &amp; = \dfrac{1}{2^k k!} \\
      m_n &amp; = \dfrac{(2k)!}{2^k k!}
      \end{align*}
      \]</span></li>
</ul>
<p>Thus, we have derived the moment list of standard normal distribution. <span class="math inline">\(\heartsuit\)</span></p>
<hr>
<p>Taylor series is such an important and useful tool, and I’ve always felt like I’ve missed out on appreciating it. Writing one blog post is not enough, but I’m looking forward to encountering it the next time I look through my math notes. :notebook:</p>
</div>
</div>
