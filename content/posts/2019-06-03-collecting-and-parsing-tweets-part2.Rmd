---
title: Collecting and parsing tweets, Part II
date: 2019-06-03
slug: collecting-and-parsing-tweets-part2
tags: ['python', 'webscraping']
series:
    - Collecting and parsing tweets
excerpt: "A three-part series explaining how I used Twitter API with python and extracted polling addresses from parsed tweets. Part II will parse the text string."
math: true
comment: true
type: "post"
bibliography: references.bib
output:
        blogdown::html_page:
                toc: true
                number_sections: true
---


Part II will clean the texts and label whether the text contains any street address. Here's a sketch of the data directory: 

```
|-- data/
     |-- raw/            # from Part I
     |-- 01_scraped/     # from Part I
     |-- 02_cleaned/     # output for Part II will go here
     |-- counts.json     # from Part I
```



We will use the following packages and set up a logger.
```{python, eval=FALSE}
import json
import os
import sys
import requests
import logging
import re
import xml.etree.ElementTree as ET
from commonregex import CommonRegex
import time

# set up logger
log = logging.getLogger(__name__)
f_handler = logging.FileHandler('{}.log'.format(__file__))
f_format = logging.basicConfig(format = '%(asctime)s - %(message)s')
f_handler.setFormatter(f_format)
log.addHandler(f_handler)
```

# Clean the text 

First, we will clean the tweet text of links and special characters. 

```{python, eval = FALSE}
def clean_tweet(tweet):
    '''
    Utility function to clean the text in a tweet by removing 
    links and special characters using regex.
    '''
    return ' '.join(re.sub("(@[\S]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())

```

The key here is the regular expression,
```
(@[\S]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)
```
which says to look for any portion of the text that are 

- @ mentions, `(@[\S]+)`; 
- emojis, `([^0-9A-Za-z \t])`; or
- url links, `(\w+:\/\/\S+)`. 

Along with this custom function, I am going to use a function from [`commonregex`](https://github.com/madisonmay/CommonRegex) called `CommonRegex()` to pick up street addresses, phone numbers, etc. in a string.

```{python, eval = FALSE}
regex_parser = CommonRegex()
```

# Find address in the text

The main function `find_address()` will use `clean_tweet()` and `regex_parser()` to clean the text and detect whether the text contains a street address. The function takes the input file, *scraped\<fromHour\>-\<toHour\>-\<page-count\>.json*, and does two things. For each dictionary in the file, the function adds two key-value items: `contain_address` whose value is logical; `clean_text` whose value is the cleaned text. It saves the value of `clean_text` into a new json file called *cleaned\<fromHour\>-\<toHour\>-\<page-count\>.json*. 


```{python, eval = FALSE}
def find_address(jsonfile):
    """
    Cleans tweet text.
    Adds entry {'contain_address': (logical)} to tweet dictionary where 
    (logical) indicates whether the cleaned tweet contains street addresses.
    ===================
    jsonfile = (str of .json file name containing the tweets)
    """
    # Load the JSON file 
    with open(os.path.join(args.script01dir, jsonfile), "r", errors = "ignore") as j:
        tweets = json.load(j)
        
    out_list = []
    c = 0 # counter for number of tweets containing address
    for tweet in tweets:
        query = clean_tweet(tweet['text'])
        tweet.update({'clean_text': query}) # add clean text to `tweet`
        # Check whether the cleaned text contains street address
        if regex_parser.street_addresses(query) == []:
            tweet.update({'contain_address': False})
        else:
            if re.search('pizza', query) != None or re.search('1 8{1-3}[0-9]{1-2}', query) != None:
                tweet.update({'contain_address': False})
                
            else:
                tweet.update({'contain_address': True})
                c += 1
                out_list.append(tweet)
    
    # Save to "cleaned*.json" file
    cleanfile = jsonfile.replace("scraped", "cleaned")
    with open(os.path.join(args.script02dir, cleanfile), "w") as j:
        json.dump(out_list, j)
    
    # Log how many tweets were found containing address
    message = "{} tweets containing address"
    log.info(message.format(c))

```

# `if __name__ == "__main__"`


```{python, eval = FALSE}

if __name__ == "__main__":

    path = os.path.join(os.getcwd(), '01_scraped')  
    for file in sorted(os.listdir(path)):
        find_address(file)
    
    
    print("Reached last file")
    sys.exit()
       

```

Here is what *cleaned201811061300-201811061400-1.json* looks like.

```
[{"created_at": "Tue Nov 06 13:08:08 +0000 2018", "text": "@PizzaToThePolls 3120 Washburn ave Minneapolis Mn https://t.co/cFGQINvuwW", "id": 1059794562317729793, "clean_text": "3120 Washburn ave Minneapolis Mn", "contain_address": true}]
```

<hr>

Next in Part III, we will geocode these street addresses with Google's Geocoding API. For how to save API keys in `.env` file, see this [tutorial](http://jonathansoma.com/lede/foundations-2019/classes/apis/keeping-api-keys-secret/).