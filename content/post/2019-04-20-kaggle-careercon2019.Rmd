---
title: "Lessons from Kaggle CareerCon 2019"
type: "post"
date: '2019-04-20'
slug: kaggle-careercon2019
tags: ['career']
comment: true
math: no
meta: true
bibliography: references.bib
excerpt: "The three day live-stream conference left me with lessons that confirmed my intuition and a general feeling of optimism."
output: 
  blogdown::html_page:
          toc: true
---

Kaggle hosted a live-stream career conference from April 16th to 18th. I was not able to join the Slack channel, but otherwise I was able to view the live stream on Kaggle's YouTube channel. I wanted to share my impressions and lessons that I learned at the events I tuned in. 

But first, I want to say that as of this writing, I am a non-STEM PhD student. I also went in to graduate program straight out of college, so I virtually have no professional work experience. I don't know how many people at the career conference were in the same situation, but I wanted to put this out to clarify my point of reference. 

[![](/post/2019-04-20-kaggle-careercon2019_files/kaggle-careercon2019.png)](https://www.kaggle.com/careercon/2019)

## Lesson: A PhD is neither necessary nor worthless.

Whether I stay or drop out, I would be coming from a non-STEM background, so I wanted to know how to promote myself amongst STEM PhD holders/dropouts while applying for data science positions. I was relieved to hear from Dr. Andrew Moore on Day 1 that it is becoming unfashionable for employers to require a PhD degree for data science positions. I was even more relieved when Jen Wang from Wayfair gave many workable tips on how to transition from PhD student to data scientist. 


Something that both Jen Wang and Allison Hegel, a PhD holder from another panel, emphasized was that the most valuable skills from our PhD training--whether it be in STEM or non-STEM field--are transferable and practical. Those who come out of PhD programs have strong character, are able to handle stress, are critical thinkers, and have strong communication skills. Also, even if you exit academia, you can find companies that provide an environment and a team structure that encourage their employees to keep learning. For instance, Jen Wang gave the example of team huddles at Wayfair which are like lab meetings in an academic research. Thus, the transition from academia to professional career as a data scientist is not only possible but can be smooth and natural.


## Lesson: Do your own projects and share them.

The single point that was most emphasized throughout Day 1 was to do your own projects and to share them. This was especially made clear by Dan Becker from Kaggle during his talk, "Why Data Science Building Skills Might Be Holding You Back (And What You Should Be Doing Instead)" where he gave the analogy through Short Term Sal and Long Term Lee. Short Term Sal only wants to invest short to midterm level of learning (i.e. taking courses and following tutorials) to get a data science job while Long Term Lee wants to invest longterm level of learning to be the best data scientist he could be when he applies for jobs. Let's assume that Sal does get a data science job earlier than Lee does, but who achieves more in ters of knowledge accumulation and attainment? Ideally, we would want Lee to achieve more than Sal. However, Dan points out that in reality, Sal would be achieving more than Lee from the very beginning and Lee would be left behind, i.e. the time to the first job matters.


To be more like Sal, we need to stop taking courses, reading textbooks, and doing kaggle competitions. Instead, we need to do our own projects and share them. This is because doing projects--i.e. beginning with data collection and cleaning data instead of using the ready-to-roll datasets on Kaggle competitions--and communicating about your findings resemble the reality of data science jobs. Fortunately, this is where I find myself to be already on track because my projects--the ones that were supposed to be my dissertation until, well, stuff happened--were exactly that. My projects are no longer viable as my dissertation, but I am motivated to finish them and to share them on this website. 


## Lesson: Product analytics sounds a lot like social science research. 

Two years ago the hottest word was "big data". Now, it's "machine learning"" and "artificial intelligence." Every day there is an article published on Medium and elsewhere about machine learning. Every other data science tweet recommended to me contains the word "machine learning." Thus, it seemed like data science jobs would look different from social science research, i.e. prediction over causal inference, machine learning instead of statistical models. However, the impression that I got from William Chen's talk ("A Behind the Scenes Look at Data Science Interviews with a Hiring Manager") and Ruben Kogel's talk ("What are Hiring Managers Really Looking For?") on Day 3 was that it is more important to be able to imagine the data generating process, which is the heart of empirical research in social science. 

For instance, according to William Chen, product analytics generally involves these broad questions:

1. What is the metric that measures success for a product/service? What are some biases that could result from using that metric? 
2. Suppose there is a sudden drop in a success metric. What could have gone wrong? What are the factor behind the sudden drop? 
3. We want to test how new features would improve the performance of this product/service in the market. What would be an appropriate experiment test? What is the success metric we want to see improve, and what are the counter metrics that we do not want to see affected by these new features? 

These are questions of empirical investigation, which product analysts and social science researchers both do. In a more familiar language to social scientists, the first question pertains to concept operationalization and measurement validity. The second is a causes-of-effects (aka reverse causal) question that asks why something happened, an answer to which would involve generating hypotheses and checking existing models. The third one is an experimental design question where the goal is to design a randomized controlled experiment that holds internal and external validity. 

If an ideal data scientist is, in the words of Ruben Kogel, an investigator who can provide insights on how the business is doing and suggest a direction for improvement in a timely and reliable fashion, then it seems that students of social science trained in causal inference are strong candidates for the job.


## Conclusion: Am I ready for a data science job? 

The three-day career conference left me with an optimistic feeling that though I lack real work experience and have yet to learn SQL, I do have a potential to be a data scinetist and that more than half of the necessary preparation has already happened through my PhD training. In particular, two features of political science research stand out as having prepared me the most for a data science career. 

First, research in political science often involves using observational data because a randomized controlled experiment in the field is either infeasible or expensive. Even if a randomized controlled experiment is possible, they suffer from low external validity because most political phenomena do not occurr randomly. States do not randomly become democratic. Politicians do not randomly choose to run for office. Governments do not randomly cut budgets and decrease funding for a program. Therefore, political science researchers most of the time have to rely on observational study, and this makes it difficult to perform concrete-proof causal inference. Under this constraint, I have learned to pay attention to details by thoroughly exploring the data before applying models. Knowing that my data is not from a random but from a selected sample keeps me humble about any findings, null or otherwise. Moreover, the challenges of messy data and important questions are what drives me. 

Second, generating good explanations about political phenomena means being able to imagine the process behind the data at hand. Some might call this 'critical thinking', but for me, 'imagination' is a better word as I am reminded of this passage from @clouds:


> The relations among these [political] events are not simply reactive, as are the encounters of physical objects; they are not readily amenable to cause-and-effect 'clocklike' models of metaphors. ... The actors in politics have memories; they learn from experience. They have goals, aspirations, calculative strategies. ... Insofar as [researchers] acknowlege the importance of scientific memory, scientific creativity, calculative strategies, goal seeking, and problem solving in their own work, they must in some degree acknowledge these qualities in the human and social material they investigate and seek to explain.

I agree with @clouds that the exercise of making predictions and identifying causal effects is ultimately founded on the humility of the researcher to respect the humanity of her subject matter, and her empathetic capacity to put herself in another person's shoes. Even though social science has the word 'science' in it and every social science discipline suffers from the obsession to imitate hard sciences, any investigation of social phenomena--whether it be academic research or product analytics--does contain an element of art. 

And acknowledging that is the first step to becoming a better investigator is what I learned through my own projects. 


## References


