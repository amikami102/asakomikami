---
title: "Collecting and parsing tweets, Part I"
date: 2019-05-29
slug: collecting-and-parsing-tweets-part1
tags: ['python', 'webscraping']
series:
    - Collecting and parsing tweets
excerpt: "A three-part series explaining how I collected tweets from Twitter API and extracted polling addresses from parsed tweets. Part I will walk through how to use Twitter API Full Archive."
math: true
comment: true
type: "post"
bibliography: references.bib
output:
        blogdown::html_page:
                toc: true
                number_sections: true
---

This is the first installment of a multi-part series waking through how I used Twitter API to collect, parse, and extract data for my project, ["Locates long polling lines with Twitter data."](https://github.com/amikami102/pizza_to_the_polls) Part I will go through how to make [a search request to the API]((https://developer.twitter.com/en/docs/tweets/search/api-reference/premium-search)) and then scrape the relevant fields of attribute from the API response object. 

The following packages will be used:

```{python, eval = FALSE}
import json
import os
import sys
import base64
import requests
import logging
```

*n.b.*: I purchased the premier tier API subscription because the sandbox/basic tier does not give you access to tweets older than 7 days. Even with premier subscription, the number of requests allowed each month can vary from 100 to 2,500 depending on what you're paying. If you are interested in gathering a large number of old tweets, I highly recommend testing your code on tweets you can still access with the free, sandbox subscription before upgrading your subscription and deploying your code with a premier account. 


# Open a Twitter developer account

Before we begin, we need a Twitter account and [sign up for an individual developer account] (https://developer.twitter.com/en/apply-for-access.html). It may take some time for the application to be accepted depending on your purpose; in my case, I had to answer an email inquiry before my application was fully accepted. Once you have a developer account, execute the first two steps on the ["Get started" page](https://developer.twitter.com/en/account/get-started).

```{r, echo = FALSE}
blogdown::shortcode('figure',  
                    src = "/post/2019-05-29-webscraping-twitter-part-i_files/twitter_developer_get-started.png", caption = "This is the page you should see when you successfully open a developer account.", label = "get-started-screenshot")
```

Once your app is created, you will have API keys and tokens.
```{r, echo = FALSE}
blogdown::shortcode('figure',
                    src = "/post/2019-05-29-webscraping-twitter-part-i_files/twitter_dev-keys-and-tokens.png",
                    caption = "You should get a set of keys and tokens associated with your app.", label = "keys-and-tokens-screenshot")
```

These API keys and tokens should be saved separately from the rest of the code script  either in a text file, in an OS environment as a variable, in your *.bash\_profile*, etc. I followed this [tutorial](http://jonathansoma.com/lede/foundations-2019/classes/apis/keeping-api-keys-secret/) and saved my credentials in a `.env` file. 


# OAuth2 authentication

In this step, we will build an authorization request to Twitter API. According to the [official documentation](https://developer.twitter.com/en/docs/basics/authentication/overview/application-only), we only need the API consumer keys to make GET requests. 

```{python, eval = FALSE}
# Load my credentials 
from dotenv import load_dotenv
load_dotenv()
client_key = os.getenv('twitter_api_key')
client_secret = os.getenv('twitter_api_secret')


# Encode my keys in base64 per instructed on 
# https://developer.twitter.com/en/docs/basics/authentication/overview/application-only
key_secret = '{}:{}'.format(client_key, client_secret).encode('ascii')
b64_encoded_key = base64.b64encode(key_secret).decode('ascii')

# Build authorization request 
auth_url = 'https://api.twitter.com/oauth2/token'
auth_headers = {
    'Authorization': 'Basic {}'.format(b64_encoded_key),
    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'}
auth_data = {'grant_type': 'client_credentials'}
auth_resp = requests.post(auth_url, headers = auth_headers, data = auth_data)
```

Before proceeding any further, I check for the status code. If the status code is "200", go ahead and store that bearer access token returned by Twitter.

```{python, eval = FALSE}
# Check status code is 200, meaning "okay"
if auth_resp.status_code != 200:
    log.error("Authorization status code: {}".format(auth_resp.status_code))
else:
    log.info("Authorization request successful")

# Store access token
access_token = auth_resp.json()['access_token']
```


# Count the number of requests to make

For my project, I needed to collect tweets posted by \@PizzaToThePolls on 2018 Midterm Election Day. Let's [count](https://developer.twitter.com/en/docs/tweets/search/api-reference/premium-search.html) the number of tweets were posted every hour by \@PizzaToThePolls on November 6th, 2018. Since I am accessing tweets that are older than 30-days, I am using the full-archive search endpoint. Initially I casted the net wide by setting `fromDate` to `201811060000` (6AM on November 6th). Upon inspection, I noticed that the account didn't show much activity in the morning, so I pushed the timestamp to `201811061100`. 

**NB**: The sandbox subscription does not include counts request in its package, so this portion is not applicable until you upgrade to premium. And even though the timeframe for both subscription tiers says "full history" like shown below, in actuality you cannot access tweets older than the last 7 days without upgrading to the premier tier. 


```{r, echo = FALSE}
blogdown::shortcode('figure', 
        src = '/post/2019-05-29-webscraping-twitter-part-i_files/twitter_dev-package-desc.png', caption = "Be careful of the misleading package description.")
```






```{python, eval = FALSE}
base_url = 'https://api.twitter.com/1.1/tweets/search/'
search_headers = {'Authorization': 'Bearer {}'.format(access_token)}
count_params = {'query': '@PizzaToThePolls',
                'fromDate': '201811061100', 
                'toDate': '201811062359',
                'bucket': 'hour'}
product = "fullarchive" # because the target tweets are older than 30days
dev_name = "research" # the dev environment name you gave

# make count request
count_url = base_url + '{}/{}/counts.json'.format(product, dev_name)
count_resp = requests.get(count_url, headers = search_headers, 
                          params = count_params)
                          
# Check status code is 200
if count_resp.status_code != 200:
    log.error("Count request status code: {}".format(count_resp.status_code))
else:
    log.info("Count data request successful")

# Save to file
with open(os.path.join("data", "counts.json"), "w") as j:
    json.dump(count_resp.json(), j)
```

This is the API response of the code above. 

```
{'results': [{'timePeriod': '201811061100', 'count': 69},
  {'timePeriod': '201811061200', 'count': 278},
  {'timePeriod': '201811061300', 'count': 655},
  {'timePeriod': '201811061400', 'count': 605},
  {'timePeriod': '201811061500', 'count': 687},
  {'timePeriod': '201811061600', 'count': 966},
  {'timePeriod': '201811061700', 'count': 2022},
  {'timePeriod': '201811061800', 'count': 2287},
  {'timePeriod': '201811061900', 'count': 2915},
  {'timePeriod': '201811062000', 'count': 3211},
  {'timePeriod': '201811062100', 'count': 5393},
  {'timePeriod': '201811062200', 'count': 5224},
  {'timePeriod': '201811062300', 'count': 4015}],
 'totalCount': 28327,
 'requestParameters': {'bucket': 'hour',
  'fromDate': '201811061100',
  'toDate': '201811062359'}}
```

The total count is $28,327$. With a premium subscription that allows for 100 requests per month and with the maximum number of tweets returned per request being 500, I can retrieve a maximum total of 50,000 tweets per month, which is more than enough to get my target tweets. 


# Retrieve tweets for every hour

We will now send a search request for tweets posted every hour from 11:00 of November 6th to 00:00:00 of November 7th. The whole code for my custom function `request_search` is shown below. I will break down the code and explain what each part is doing. 

```{python, eval = FALSE}
def request_search(fromHour, toHour, next_, pgcount):
    """
    This function collects tweets posted by @PizzaToThePolls on
    2018 Election Day during the hour 'fromDate' to 'toDate'.
    Some hours are busier than others and will require multiple responses to 
    cover all the data pages.
    ======================
    fromDate = 'YYYYMMDDHHMM'
    toDate = 'YYYYMMDDHHMM'
    pgcount = (int, page counter)
    next_ = (str token for the next page of search result)
    """

    
    # set the search parameters
    search_params = {'query': '@PizzaToThePolls',
                     'fromDate': fromDate, 'toDate': toDate,
                     'maxResults': 500, 
                     'next': next_}
    if next_ == 0:
        del search_params['next']
        
        
    # Make search request
    search_url = '{}fullarchive/research.json'.format(base_url)
    search_resp = requests.get(search_url, headers = search_headers, 
                               params = search_params)
    
    
    # Check status code is 200
    if search_resp.status_code != 200:
        log.error("Request status code: {}".format(search_resp.status_code))
    else:
        log.info("Request successful")
        
    search_data = search_resp.json()
	
	  # Save to file
    filename = fromDate + "-" + toDate + "-" + str(pgcount) 
    with open(os.path.join('data/raw', "search_resp" + filename + ".json"), "w") as j:
        json.dump(search_data, j)
    
    # Scrape the date created, text, and id of each tweet 
    # in the search response. Add the dictionary to output list.
    request_out = []
    for res in search_data['results']:
        t = {'created_at': res['created_at'], 
             'text': res['text'], 
             'id': res['id']}
        request_out.append(t)
    
    # Save to file
    with open(os.path.join('data/01_scraped', "scraped" + filename +".json"), "w") as j:
        json.dump(request_out, j)
    
    # Get 'next' token for the next page if exists, else stop.
    if 'next' not in search_data:
        log.info("We've reached the last page")
        return(0)
    else:
        return(search_data['next'])
    
```


## Make search request

All the arguments except for `pgcount` are fed into `search_params` to build a search query. The first time we make a request for tweets between `fromHour` to `toHour`, the argument `next_` is set to the default value of 0. Depending on how many tweets were posted in the hour interval, there may be multiple pages of results. If there is a next page, the JSON response will have a field called `next`, which will be the new value for `next_`. The raw JSON response is saved as JSON files in a subfolder, `data/raw`.


```{python, eval = FALSE}
    search_params = {'query': '@PizzaToThePolls',
                     'fromDate': fromHour, 
                     'toDate': toHour,
                     'maxResults': 500}
    if next_ != 0:
        search_params['next'] = next_
    
    # make search request
    base_url = 'https://api.twitter.com/1.1/tweets/search/'
    product = "fullarchive"
    dev_name = "research" # the dev environment name 
    search_url = base_url + '{}/{}.json'.format(product, dev_name)
    search_resp = requests.get(search_url, headers = search_headers, 
                               params = search_params)
    # Check status code is 200
    if search_resp.status_code != 200:
        log.error("Request status code: {}".format(search_resp.status_code))
    else:
        log.info("Request successful")
        
    search_data = search_resp.json()
    
    # Save to file
    filename = fromHour + "-" + toDate + "-" + str(pgcount) 
    with open(os.path.join("data/raw", "search_resp" + filename + ".json"), "w") as j:
        json.dump(search_data, j)
```


## Scrape the response JSON

Most of the elements in the raw response is superfluous, so it is better to scrape the relevant fields---the time at which the tweet was written, the text, and the ID--- into a dictionary object and append it to an output list, which is saved as a json file. These json files are saved to a subfolder, You can view a sample of response JSON in the official documentation: https://developer.twitter.com/en/docs/tweets/search/api-reference/get-search-tweets.")


```{python, eval = FALSE}
    request_out = [] 
    for res in search_data['results']:
        t = {'created_at': res['created_at'], 
             'text': res['text'], 
             'id': res['id']}
        request_out.append(t)
        
    # Save to file
    with open(os.path.join("data/01_scraped" + filename +".json"), "w") as j:
        json.dump(request_out, j)
```


## Pull the next page token

At the end of the function, we check whether the JSON response contains a 'next' token. If there is, the function returns the `next` token; otherwise, it returns `0`.

```{python, eval = FALSE}
    # Get 'next' token for the next page if exists, else stop.
    if 'next' not in search_data:
        log.info("We've reached the last page")
        return(0)
    else:
        return(search_data['next'])
```


# `if __name__ == "__main__"`

Finally, we will run the whole module as a script. Here is where `pgcount` comes into play. `pgcount` marks which page of results we are currently on. See the section 'Pagination' in the API documentation: https://developer.twitter.com/en/docs/tweets/search/api-reference/premium-search.html.") The counter starts at `0` and is updated every time there is a next page. 


```{python, eval = FALSE}
if __name__ == "__main__":
    
    with open(os.path.join("data", "counts.json"), "r") as j:
       counts = json.load(j)['results']
    for count in counts:
       fromHour = count['timePeriod']
       toHour = str(int(fromHour) + 100)
       c = 0
       to_next = request_search(fromHour, toHour, pgcount = c, next_ = 0)
       while(to_next != 0):
           c += 1
           to_next = request_search(fromHour, toHour, pgcount = c, next_ = to_next)
    sys.exit()
```



Here is a glimpse of what *scraped201811061100-201811061200-0.json* looks like. The JSON file contains the first page of the API response results for tweets posted between 11am and 12pm.

```{python, eval = FALSE}
with open("data/scraped/scraped201811061100-201811061200-0.json", "r") as j:
    res = json.load(j)[:5]
```

```
[{'created_at': 'Tue Nov 06 11:59:40 +0000 2018',
  'text': 'RT @CarolineAFish: Helpful information via @tinaLillet for voting in New York City TODAY. Most importantly, @PizzaToThePolls 😉🍕🍕#GetOutTheV…',
  'id': 1059777331026448384},
 {'created_at': 'Tue Nov 06 11:59:03 +0000 2018',
  'text': 'RT @wserrino: Pizza to the Polls  -- If People are waiting in long lines - help them stay happy and stay to vote by contacting @pizzatothep…',
  'id': 1059777175656783877},
 {'created_at': 'Tue Nov 06 11:57:02 +0000 2018',
  'text': 'RT @PizzaToThePolls: @AnnaSekiguchi 16 pizzas from dominos should be on the way!',
  'id': 1059776669307813888},
 {'created_at': 'Tue Nov 06 11:56:24 +0000 2018',
  'text': 'RT @PizzaToThePolls: Happy voting everyone!! Here’s hoping your voting journey is quick and easy today but if you get stuck in line, we’re…',
  'id': 1059776509693448192},
 {'created_at': 'Tue Nov 06 11:55:05 +0000 2018',
  'text': "It's election day.   If you are in line when the polls close, stay in line.   You still get to vote.   If the lines… https://t.co/TGpGjLCIIu",
  'id': 1059776179366965248}]
```

<hr>

Next in Part II, we will be extracting polling addresses from these tweets!:round_pushpin:



