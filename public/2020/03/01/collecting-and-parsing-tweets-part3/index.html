<!DOCTYPE html>
<html
  class=""
  lang="en-us"
  prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"
>
  <head>
    <meta charset="utf-8" />

    

  <ul>
    
      <li>
          <a href="../../../../tags/python/">python</a>
        </li>
      <li>
          <a href="../../../../tags/webscraping/">webscraping</a>
        </li></ul>


    <title>Collecting and parsing tweets, Part III</title>
    <link rel="canonical" href="../../../../2020/03/01/collecting-and-parsing-tweets-part3/" />

    

    <link rel="stylesheet" href="../../../../css/reboot.css" />
<link rel="stylesheet" href="../../../../css/style.css" />
<script type="text/javascript" src="../../../../js/main.js"></script>


<link rel="stylesheet" href="../../../../highlight/styles/github-gist.css" rel="stylesheet" id="theme-stylesheet">
<script src="../../../../highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
</script>



<link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=DM+Sans&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;700;900&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Barlow+Condensed:wght@600&display=swap" rel="stylesheet">
    <link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="../../../../apple-touch-icon.png" />
  </head>


<body lang="en-us">
  <div class="nav-bkg">
    <nav class="content-container pagewide-bar-padding">
      <span class="divider">/ </span>
      <a href="../../../../" >Asako Mikami</a>
      <ul class="list-unstyled right-links">

          <li>
            <a href="../../../../about/">
              <span class="post-title">About</span>
            </a>
          </li>

          <li>
            <a href="../../../../post/">
              <span class="post-title">All posts</span>
            </a>
          </li>

          <li>
            <a href="../../../../series/">
              <span class="post-title">Series</span>
            </a>
          </li>

</ul>

    </nav>
  </div>

  <section id="main" class="content-container look-sheet article-pad-v " itemprop="mainEntityOfPage">
    <h1 itemprop="name" id="title">Collecting and parsing tweets, Part III</h1>

    <article itemprop="articleBody" id="content" class="article-body">
      

<div id="TOC">
<ul>
<li><a href="#get-api-key"><span class="toc-section-number">1</span> Get API key</a></li>
<li><a href="#pass-address-to-googles-geocoding-api"><span class="toc-section-number">2</span> Pass address to Google’s Geocoding API</a></li>
<li><a href="#parse-xml"><span class="toc-section-number">3</span> Parse XML</a></li>
<li><a href="#if-__name__-__main__"><span class="toc-section-number">4</span> <code>if __name__ == &quot;__main__&quot;</code></a></li>
</ul>
</div>

<p>Part II of this tutorial series left us with <em>cleaned&lt;fromHour&gt;-&lt;toHour&gt;-&lt;page-count&gt;.json</em> files. Part III will parse the street addresses in those files and geocode them through Google’s Geocoding API.</p>
<p>Here is the directory tree for Part III:</p>
<pre><code>|-- data/
     |-- raw/          # from Part I
     |-- 01_scraped/   # from Part I
     |-- 02_cleaned/   # from Part II        
     |-- 03_parsed/    # output for Part III will go here
     |-- counts.json   # from Part I</code></pre>
<p>Here are the packages and logger setup:</p>
<pre class="python"><code>import os
import requests
from requests.exceptions import HTTPError
import json
import sys
from commonregex import CommonRegex
import logging
import re
import argparse
import xml.etree.ElementTree as ET
import time

# set up logging
log = logging.getLogger(__name__)
log.setLevel(logging.ERROR)
if args.verbose:
    log.setLevel(logging.DEBUG)
loghandler = logging.StreamHandler(sys.stderr)
loghandler.setFormatter(logging.Formatter(&quot;[%(asctime)s] %(message)s&quot;))
log.addHandler(loghandler)</code></pre>
<div id="get-api-key" class="section level1">
<h1><span class="header-section-number">1</span> Get API key</h1>
<p>Google’s Geocoding API allows users to convert addresses to geocoordinates and parses the address. To get the API key,</p>
<ol style="list-style-type: decimal">
<li><a href="https://developers.google.com/maps/gmp-get-started">create an account and a project</a> on Google Maps Platform and enable Geocoding API for your project;</li>
<li>from you Console page, <a href="https://developers.google.com/maps/documentation/geocoding/get-api-key">get the API key</a>.</li>
</ol>
<p>I use <a href="http://jonathansoma.com/lede/foundations-2019/classes/apis/keeping-api-keys-secret/"><code>.env</code> file</a> to store my API keys as I have done with my Twitter API credentials.</p>
<pre class="python"><code>from dotenv import load_dotenv
load_dotenv()
google_api_key = os.getenv(&#39;google_api_key&#39;)</code></pre>
</div>
<div id="pass-address-to-googles-geocoding-api" class="section level1">
<h1><span class="header-section-number">2</span> Pass address to Google’s Geocoding API</h1>
<p>The <code>get_address</code> function passes the tweet text to Google’s Geocoding API and returns the API response. Recall that in Part II we added a dictionary item that indicates whether the tweet text contains an address. If the value for <code>contain_address</code> is <code>True</code>, the text is passed to Geocoding API. The API response is returned in XML format.</p>
<pre class="python"><code>&quot;&quot;&quot; STEP 1: geocode and parse&quot;&quot;&quot;

def get_address(tweet):
    &quot;&quot;&quot;
    Pass the tweet text to Google&#39;s Geocoding API to get structured 
    address data in xml format. 
    &quot;&quot;&quot;
    with open(&quot;script/credentials/google_credentials.json&quot;, &quot;r&quot;) as c:
        api_key = json.load(c)[&#39;geocode_api&#39;]
    
    
    base_url = &quot;https://maps.googleapis.com/maps/api/geocode/xml?address&quot;
    if tweet[&#39;contain_address&#39;] == True:
       query = tweet[&#39;clean_text&#39;]
       req_url = base_url + &quot;=&quot; + query + &#39;&amp;lang=en&amp;key=&#39; + google_api_key
       try:
           resp = requests.get(req_url)
           # If the response was successful, no Exception will be raised
           resp.raise_for_status()
       except HTTPError as http_err:
           log.info(f&#39;HTTP error occurred: {http_err}&#39;) 
       except Exception as err:
           log.info(f&#39;Other error occurred: {err}&#39;) 
       else:
           log.info(&#39;Success!&#39;)
    else:
       resp = None
       
    
    return resp</code></pre>
</div>
<div id="parse-xml" class="section level1">
<h1><span class="header-section-number">3</span> Parse XML</h1>
<p>The next function <code>parse_address</code> takes the XML object returned by <code>get_address</code> and parses it using <code>xml.etree.ElementTree</code> functions. After studying the API documentation, I decided to specify <a href="https://developers.google.com/maps/documentation/geocoding/intro#Types">the types of address results</a> because sometimes the text I was sending to the API in <code>get_address</code> function was not the full street address but just the city and the state. Defining <code>type_list</code> allowed me to filter out results that did not give me an exact polling location. It is not intuitive what address types your addresses belong to. For instance, when I only specified my result type to be <code>street_address</code>, I was only able to get 10 or so unique addresses. It was only after I got all the unique address types in my data that I was able to see that most polling location addresses were of other address types.</p>
<pre class="python"><code>def parse_address(resp):
    &quot;&quot;&quot;
    Parses the `get_address` response output and constructs a
    dictionary. 
    =====================
    resp (the response xml output of `get_address`)
    &quot;&quot;&quot;
    # set up tree
    root = ET.fromstring(resp.content)
    
    # only parse if google API response is &#39;ok&#39; and 
    # address type is in the `type_list`
    type_list = [&#39;street_address&#39;,
                 &#39;church&#39;,
                 &#39;locality&#39;,
                 &#39;clothing_store&#39;,
                 &#39;subpremise&#39;,
                 &#39;colloquial_area&#39;,
                 &#39;premise&#39;,
                 &#39;establishment&#39;,
                 &#39;bar&#39;]
    
    if (root.find(&#39;status&#39;).text.lower() == &quot;ok&quot; and 
        root.find(&#39;./result/type&#39;).text in type_list):
        add_type = root.find(&#39;./result/type&#39;).text
        path = &quot;./result/[type=&#39;{}&#39;]/&quot;.format(add_type)
        try:
            full = root.find(path + &quot;formatted_address&quot;).text
        except UnboundLocalError:
            full = None
        try:
            st_num = root.find(path + &quot;address_component/[type=&#39;street_number&#39;]/long_name&quot;).text
        except:
            st_num = None
        try:
            route = root.find(path + &quot;address_component/[type=&#39;route&#39;]/long_name&quot;).text
        except:
            route = None
        try:
            city = root.find(path + &quot;address_component/[type=&#39;locality&#39;]/long_name&quot;).text
        except:
            try:
                city = root.find(path + &quot;address_component/[type=&#39;sublocality&#39;]/long_name&quot;).text
            except:
                city = None
        try:
            county = root.find(path + &quot;address_component/[type=&#39;administrative_area_level_2&#39;]/long_name&quot;).text
        except:
            county = None
        try:
            state = root.find(path + &quot;address_component/[type=&#39;administrative_area_level_1&#39;]/long_name&quot;).text
            state_abbv = root.find(path + &quot;address_component/[type=&#39;administrative_area_level_1&#39;]/short_name&quot;).text
        except:
            state = state_abbv = None
        try:
            zipcode = root.find(path + &quot;address_component/[type=&#39;postal_code&#39;]/short_name&quot;).text
        except:
            zipcode = None
        try:
            lat = root.find(path + &quot;geometry/location/lat&quot;).text
            lng = root.find(path + &quot;geometry/location/lng&quot;).text
        except:
            lat = lng = None
    else:
        full = st_num = route = city = county = state = state_abbv = zipcode = lat = lng = None
    # construct dictionary 
    address = {&quot;full&quot;: full,
                &quot;st_num&quot;: st_num, 
               &quot;route&quot;: route,
                &quot;city&quot;: city,
                &quot;county&quot;: county,
                &quot;state&quot;: state,
                &quot;state_abbv&quot;: state_abbv,
                &quot;zipcode&quot;: zipcode,
                &quot;lat&quot;: lat,
                &quot;lng&quot;: lng}
    return address</code></pre>
</div>
<div id="if-__name__-__main__" class="section level1">
<h1><span class="header-section-number">4</span> <code>if __name__ == &quot;__main__&quot;</code></h1>
<p>The main function iterates over files in <em>data/02_cleaned</em>, and the outputs (a list of updated tweet dictionaries) are saved as json files in <em>data/03_parsed</em>.</p>
<pre class="python"><code>if __name__ == &quot;__main__&quot;:

    # iterate over files in &#39;data/02_cleaned&#39;
    for file in sorted(&#39;data/02_cleaned&#39;):
        with open(os.path.join(path, file), &quot;r&quot;) as j:
            tweets = json.load(j)
        for tweet in tweets:
            xml_resp = get_address(tweet) 
            if xml_resp is not None:
                address_dict = parse_address(xml_resp)
                tweet.update(address_dict)
            else:
                pass
        time.sleep(2.5)
        parsed = file.replace(&quot;cleaned&quot;, &quot;parsed&quot;)
        with open(os.path.join(&#39;data/03_parsed&#39;, parsed), &quot;w&quot;) as j:
            json.dump(tweets, j)
    log.message(&#39;Reached last file&#39;)
    sys.exit()</code></pre>
<p>This is what <em>parsed201811061300-201811061400-1.json</em> looks like:</p>
<pre><code>[{&quot;created_at&quot;: &quot;Tue Nov 06 13:08:08 +0000 2018&quot;, &quot;text&quot;: &quot;@PizzaToThePolls 3120 Washburn ave Minneapolis Mn https://t.co/cFGQINvuwW&quot;, &quot;id&quot;: 1059794562317729793, &quot;clean_text&quot;: &quot;3120 Washburn ave Minneapolis Mn&quot;, &quot;contain_address&quot;: true, &quot;full&quot;: &quot;3120 Washburn Ave N, Minneapolis, MN 55411, USA&quot;, &quot;st_num&quot;: &quot;3120&quot;, &quot;route&quot;: &quot;Washburn Avenue North&quot;, &quot;city&quot;: &quot;Minneapolis&quot;, &quot;county&quot;: &quot;Hennepin County&quot;, &quot;state&quot;: &quot;Minnesota&quot;, &quot;state_abbv&quot;: &quot;MN&quot;, &quot;zipcode&quot;: &quot;55411&quot;, &quot;lat&quot;: &quot;45.0128527&quot;, &quot;lng&quot;: &quot;-93.3169213&quot;}]
</code></pre>
<hr>
<p>This completes the tutorial series, “Collecting and Parsing Tweets!” I hope it helps you navigate Twitter and Google API’s. The series was based on my project, “Locating Long Polling Lines with Twitter Data.” The python scripts that each part of the tutorial was based on can be found at <a href="https://github.com/amikami102/pizza_to_the_polls">my Github repostiory</a>:</p>
<ul>
<li><strong>Part I</strong>: [tutorial]({{&lt; ref “/content/collecting-and-parsing-tweets-part1” &gt;}}) / <a href="https://github.com/amikami102/pizza_to_the_polls/blob/master/script/scrape_tweets/01_scrape_twitter.py">.py script</a></li>
<li><strong>Part II</strong>: [tutorial]({{&lt; ref “/content/collecting-and-parsing-tweets-part2” &gt;}}) / <a href="https://github.com/amikami102/pizza_to_the_polls/blob/master/script/scrape_tweets/02_clean_and_find_address.py">.py script</a></li>
<li><strong>Part III</strong>: [tutorial]({{&lt; ref “/content/collecting-and-parsing-tweets-part3” &gt;}}) / <a href="https://github.com/amikami102/pizza_to_the_polls/blob/master/script/scrape_tweets/03_parse_address.py">.py script</a></li>
</ul>
</div>

    </article>
  </section>
  

  </body>
</html>
