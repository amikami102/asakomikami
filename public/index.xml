<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Asako Mikami</title>
    <link>/</link>
    <description>Recent content on Asako Mikami</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 Apr 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Chelsea E. Carter, &#34;Forts and Frontier: The U.S. Army and the Spatial Distribution of Population&#34;</title>
      <link>/2020/04/18/jmp-review-chelsea-carter/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/18/jmp-review-chelsea-carter/</guid>
      <description>This blog post is part of my ‚ÄúJMP by Women in Econ‚Äù series where I summarize and review some of the job market papers written by women PhD candidates. The papers are selected from Jennifer Doleac‚Äôs Twitter thread. I am not reading this from the perspective of an economist, let alone of a job committee member: I just like reading cool articles that use observational data to tackle challenging research problems.</description>
    </item>
    
    <item>
      <title>Yi Cheng, &#34;The Unexpected Costs of Expertise: Evidence from Highly Specialized Physicians&#34;</title>
      <link>/2020/04/10/jmp-review-yi-cheng/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/04/10/jmp-review-yi-cheng/</guid>
      <description>This blog post is part of my ‚ÄúJMP by Women in Econ‚Äù series where I summarize and review some of the job market papers written by women PhD candidates. The papers are selected from Jennifer Doleac‚Äôs Twitter thread. I am not reading this from the perspective of an economist, let alone of a job committee member: I just like reading observational studies that tackle challenging research problems.:bowtie:
For my first post of this series, I am reviewing Yi Cheng‚Äôs job market paper, ‚ÄúThe Unexpected Costs of Expertise: Evidence from Highly Specialized Physicians‚Äù(November 4, 2019).</description>
    </item>
    
    <item>
      <title>Collecting and parsing tweets, Part III</title>
      <link>/2020/03/01/collecting-and-parsing-tweets-part3/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/01/collecting-and-parsing-tweets-part3/</guid>
      <description>1 Get API key 2 Pass address to Google‚Äôs Geocoding API 3 Parse XML 4 if __name__ == &amp;quot;__main__&amp;quot;   Part II of this tutorial series left us with cleaned&amp;lt;fromHour&amp;gt;-&amp;lt;toHour&amp;gt;-&amp;lt;page-count&amp;gt;.json files. Part III will parse the street addresses in those files and geocode them through Google‚Äôs Geocoding API.
Here is the directory tree for Part III:
|-- data/ |-- raw/ # from Part I |-- 01_scraped/ # from Part I |-- 02_cleaned/ # from Part II |-- 03_parsed/ # output for Part III will go here |-- counts.</description>
    </item>
    
    <item>
      <title>Collecting and parsing tweets, Part II</title>
      <link>/2019/06/03/collecting-and-parsing-tweets-part2/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/06/03/collecting-and-parsing-tweets-part2/</guid>
      <description>1 Clean the text 2 Find address in the text 3 if __name__ == &amp;quot;__main__&amp;quot;   Part II will clean the texts and label whether the text contains any street address. Here‚Äôs a sketch of the data directory:
|-- data/ |-- raw/ # from Part I |-- 01_scraped/ # from Part I |-- 02_cleaned/ # output for Part II will go here |-- counts.json # from Part I We will use the following packages and set up a logger.</description>
    </item>
    
    <item>
      <title>Collecting and parsing tweets, Part I</title>
      <link>/2019/05/29/collecting-and-parsing-tweets-part1/</link>
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/29/collecting-and-parsing-tweets-part1/</guid>
      <description>1 Open a Twitter developer account 2 OAuth2 authentication 3 Count the number of requests to make 4 Retrieve tweets for every hour 4.1 Make search request 4.2 Scrape the response JSON 4.3 Pull the next page token  5 if __name__ == &amp;quot;__main__&amp;quot;   This is the first installment of a multi-part series waking through how I used Twitter API to collect, parse, and extract data for my project, ‚ÄúLocates long polling lines with Twitter data.</description>
    </item>
    
    <item>
      <title>The Central Limit Theorem As I Understand It</title>
      <link>/2019/05/19/central-limit-theorem/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/19/central-limit-theorem/</guid>
      <description>1 The Central Limit Theorem for IID Samples 1.1 The Fetus 1.2 The Baby CLT 1.3 The Adult CLT, v1.0 1.4 The Adult CLT, v2.0  2 The Central Limit Theorem for Non-IID Samples 2.1 Are Agricultural Crop Yields Normally Distributed?  References   According to Salsburg (2001), the Central Limit Theorem (CLT) didn‚Äôt have an explicit proof when it was first written down by Abraham de Moivre in the 18th century.</description>
    </item>
    
    <item>
      <title>Taylor Series in Probability</title>
      <link>/2019/05/08/taylor-series/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/08/taylor-series/</guid>
      <description>1 Definition 1.1 Taylor Series of Common Functions  2 Interesting Properties 2.1 Taylor Series Uniqueness 2.2 Taylor‚Äôs Theorem  3 Application 3.1 Moment Generating Property 3.2 Mgf of Poisson Distribution 3.3 Moment List of Standard Normal Dist.    I first learned about Taylor series in high school, but I didn‚Äôt quite master series expansion in general, let alone Taylor. Even after several math courses in college, my familiarity with it has barely grown since our first encounter back in high school.</description>
    </item>
    
    <item>
      <title>Collecting historical weather data, Part III</title>
      <link>/2019/05/04/weather-data-part3/</link>
      <pubDate>Sat, 04 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/04/weather-data-part3/</guid>
      <description>1 Cleaning the data 2 Visualizing the data References   To recount, Part II interpolated the precipitation data we obtained in Part I in order to get county-level precipitation values. Part III (this post) will clean the data further in preparation for the visualization stage and then create a gif animation of chloreoploth maps plotting precipitation level in inches over the seven days leading up to the 2006 midterm election.</description>
    </item>
    
    <item>
      <title>Geometric and triangle inequalities</title>
      <link>/2019/05/03/geometric-and-triangle-inequality/</link>
      <pubDate>Fri, 03 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/03/geometric-and-triangle-inequality/</guid>
      <description>Geometric inequality Triangle inequality   The last post, ‚ÄúCorrelation as standardized covariance‚Äù, mentions geometric and triangle inequalities, i.e.
 geometric inequality: \(\left| ab \right| \leq \dfrac{a^2 + b^2}{2}\) for all \(a, b \in \mathbb{R}\) triangle inequality for expectations: \(\left| \E [X] \right| \leq \E [ \left| X \right|]\) for a discrete or continuous random variable \(X\).  They look deceptively simple and intuitive, but I was surprised by how frequently they can be used in proofs.</description>
    </item>
    
    <item>
      <title>Correlation is just standardized covariance</title>
      <link>/2019/05/01/correlation-is-just-standardized-covariance/</link>
      <pubDate>Wed, 01 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/01/correlation-is-just-standardized-covariance/</guid>
      <description>Sometimes, we re-learn a concept in a new way that opens our eyes and feel like we finally get it. This happened to me with correlation: the correlation of two random variables is just standardized covariance!
n.b.: I learned the material in this article from taking Professor Daniel Weiner‚Äôs course, Mathematics of Statistics at BU. All the credit of this post goes to him while all mistakes are mine. üíÅ</description>
    </item>
    
    <item>
      <title>Lessons from Kaggle CareerCon 2019</title>
      <link>/2019/04/20/kaggle-careercon2019/</link>
      <pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/20/kaggle-careercon2019/</guid>
      <description>Lesson: A PhD is neither necessary nor worthless. Lesson: Do your own projects and share them. Lesson: Product analytics sounds a lot like social science research. Conclusion: Am I ready for a data science job? References   Kaggle hosted a live-stream career conference from April 16th to 18th. I was not able to join the Slack channel, but otherwise I was able to view the live stream on Kaggle‚Äôs YouTube channel.</description>
    </item>
    
    <item>
      <title>Collecting historical weather data, Part II</title>
      <link>/2019/04/18/weather-data-part2/</link>
      <pubDate>Thu, 18 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/18/weather-data-part2/</guid>
      <description>1 Reproject shapefile 2 Perform Kriging interpolation 3 Reproject raster layer 4 Obtain average total rainfall for each county References   prcp_data we created in Part I is not useful in and of itself because it only shows precipitation values recorded by weather stations. Ultimately, I want to be able to say that county \(i\) for \(i = 1, \ldots, n\) on day \(t\) for \(t = 1, \ldots, T\) had \(x_{it}\) level of precipitation.</description>
    </item>
    
    <item>
      <title>Migrating to Hugo</title>
      <link>/2019/04/16/migrating-to-hugo/</link>
      <pubDate>Tue, 16 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/16/migrating-to-hugo/</guid>
      <description>1 Installing ‚Äòtuftesque‚Äô theme 2 Setting up a navigation bar 2.1 nav element 2.1.1 .navbar-brand class  2.2 button element 2.3 .collapse.navbar-collapse class 2.4 Adding padding to the body  3 Adding syntax highlighting   I moved this blog from Jekyll to Hugo because I felt like I was missing out on blogdown. I spent days browsing for that right theme all over https://themes.gohugo.io/, but I finally made up my mind to use Nick Strayer‚Äôs tuftesque, which was one of Yihui Xie‚Äôs honorable mentions for impressive blogdown implementation.</description>
    </item>
    
    <item>
      <title>Collecting historical weather data, Part I</title>
      <link>/2019/04/10/weather-data-part1/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/10/weather-data-part1/</guid>
      <description>1 Obtain NOAA API key token 2 Obtain weather data with rnoaa::ncdc() 2.1 Code breakdown  3 Obtain weather station data with rnoaa::ncdc_stations() 4 Join dataframes 5 Create spatial point dataframes 6 (Optional) Plot the data   As part of a coauthored project investigating whether rain deters voters from going to the polls in US elections, I had to collect past weather data. This series walks you through how to gather data on the daily total precipitation level from November 1st to November 7th, 2006, where the last day is the 2006 midterm election.</description>
    </item>
    
    <item>
      <title>Introducing a replication project of Warren (2019)</title>
      <link>/2019/04/02/introduce-replicate-warren/</link>
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/04/02/introduce-replicate-warren/</guid>
      <description>I have a project called replicate_warren (a private Github repository), where I am trying to replicate a sociology study, Warren (2019). To put it succinctly, Warren (2019) collected data from r√©sum√©s of sociology PhD graduates and measured how over-achieving recent PhD graduates had to be to land a junior faculty jobs at top sociology departments in the United States. I want to replicate the study in the political science field.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>My name is Asako. I am a PhD student in political science at Boston University. I received my BA from Smith College where I majored in Government and minored in Mathematics.
I have been studying political science for almost 9 years, but during my PhD training, I have found my calling in data science and am looking to start my career in the field.
My blog showcases my working projects, R/Python tutorials for data collection, and notes.</description>
    </item>
    
  </channel>
</rss>