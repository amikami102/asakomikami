---
title: "Collecting historical weather data, Part III"
date: '2019-05-04'
slug: weather-data-part3
draft: false
categories: ['weather data', 'GIS', 'R']
excerpt: "We're going to end this series with a blast and create an animated map."
math: true
meta: true
comment: true
bibliography: references.bib
output:
        blogdown::html_page:
                toc: true
                number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

To recount, [Part II](`r blogdown::shortcode('ref', '/post/2019-04-18-weather-data-part2')`) interpolated the precipitation data we obtained in Part I in order to get county-level precipitation values. Part III (this post) will clean the data further in preparation for the visualization stage and then create a gif animation of chloreoploth maps plotting precipitation level in inches over the seven days leading up to the 2006 midterm election. 



*n.b.*: I am back to using R as I did in [Part I](`r blogdown::shortcode('ref', '/post/2019-04-10-weather-data-part1')`). I could not and still cannot install `geopandas`, a python package for GIS data analysis and visualization, on my MacOS. What I glossed from reading the discussions [here](https://stackoverflow.com/questions/34427788/how-to-successfully-install-pyproj-and-geopandas), [here](https://github.com/geopandas/geopandas/issues/237), and [here](https://www.youtube.com/watch?v=UA8Zp7tdmjs) is that `geopandas` depends on older version of existing packages and switching to older versions causes complications. 



# Cleaning the data

There are three things I need to do to clean the data:

1. recode negative precipitation values to 0;
2. convert units from tenths of millimeter to inches;
3. attach 5-digit FIPS code label to each county. 

The goal is to produce seven dataframe objects, each with the following attributes. 
```
GEO_ID (string, unique id for each county)
rainfall (numeric, average precipitation level)
fips_code (factor, 5-digit FIPS code)
```

During [the interpolation process](`r blogdown::shortcode('ref', '/post/2019-04-18-weather-data-part2')`), some of the counties received negative values because `arcpy.UniversalKriging()` did not allow us to set a lower bound at 0. This is one reason why I wonder if universal Kriging may be too complex for precipitation data. 

I will be recoding any negative values to zero. According to the [README documentation for GHCN Daily Ver 3.24](https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt), the precipitation level is recorded in tenths of mm. I will be converting the unit to inches by dividing the values by 254.


First, load the libraries. 
```{r, eval = FALSE}
library(rio)
library(tidyverse)
```

Second, I also create a dataframe called `counties10`, which is basically the attribute data portion of the spatial polygon dataframe contained in [`gz_2010_us_050_00_500k.shp`](https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.2010.html), the 2010 cartographic boundary file for US counties published by the US Census Bureau. Note that this is the same cartographic boundary file I used in [Part II ](https://asakomikami.com/2019/04/18/weather-data-part2).


```{r, eval = FALSE}
## download and extract zip file from 2010 Census
base_url <- "http://www2.census.gov/geo/tiger/GENZ2010/"
zipname <- "gz_2010_us_050_00_500k"

zipname %>% map(~ {
        zipfile_url <- paste0(base_url, zipname, ".zip")
        temp <- tempfile()
        download.file(zipfile_url, temp)
        unzip(temp, exdir = zipname)
        unlink(temp)
})
counties10 <- readOGR(zipname, "gz_2010_us_050_00_500k")@data %>%
        select(GEO_ID, NAME, STATE, COUNTY)
```

Here, `GEO_ID` is the unique identifier for each county; `STATE` and `COUNTY` give the FIPS code for the state and county respectively.

```
> head(counties10)
          GEO_ID      NAME STATE COUNTY
0 0500000US01029  Cleburne    01    029
1 0500000US01031    Coffee    01    031
2 0500000US01037     Coosa    01    037
3 0500000US01039 Covington    01    039
4 0500000US01041  Crenshaw    01    041
5 0500000US01045      Dale    01    045
```

I will be using *county-rainfall\<date\>.dbf* files produced in [Part II]((`r blogdown::shortcode('ref', "/post/2019-04-18-weather-data-part2")`). Recall that these dbf files contain the statistics of precipitation data for each county, but I am only going to use the mean statistic. Here are the first several rows of *county-rainfall2006-11-01.dbf*. 

```
> head(import('county-rainfall2006-11-01.dbf'))
          GEO_ID ZONE_CODE COUNT      AREA
1 0500000US01029         1  1447 1.447e+09
2 0500000US01031         2  1762 1.762e+09
3 0500000US01037         3  1728 1.728e+09
4 0500000US01039         4  2702 2.702e+09
5 0500000US01041         5  1580 1.580e+09
6 0500000US01045         6  1458 1.458e+09
          MIN        MAX      RANGE         MEAN
1  9.80359554 36.2611084 26.4575129 17.396314212
2  0.00000000  0.2362743  0.2362743  0.018611072
3 -0.02778177 33.7240906 33.7518723 11.776352534
4 -0.01088683  0.2124358  0.2233227  0.004046564
5  0.00000000  2.7885394  2.7885394  0.606575074
6  0.00000000  0.2863130  0.2863130  0.093945961
         STD         SUM
1 7.04554499 25172.46666
2 0.04843627    32.79271
3 8.81281628 20349.53718
4 0.02082290    10.93382
5 0.75927404   958.38862
6 0.09835679   136.97321
```

The following function will create *county-rainfall\<date\>.rds*. 

```{r, eval = FALSE}
clean_rainfall <- function(dbf_file){
        #-----------------------------
        # Load .dbf file and clean the data by
        # 1.) recoding negative rainfall values to 0,
        # 2.) convert the unit to inches, and
        # 3.) add FIPS code label to each county. 
        #-----------------------------
        # dbf_file (str, name of the dbf file)
        #-----------------------------
        
        df <- dbf_file %>% import() %>%
                # adjust the levels of GEO_ID to match that of the counties10
                mutate(GEO_ID = factor(GEO_ID, 
                                levels(counties10$GEO_ID)), 
                       # recode to 0 if rainfall level is negative
                       MEAN = ifelse(MEAN < 0, 0, MEAN),
                       # convert unit to inches 
                       rainfall = MEAN/254) %>%
                select(GEO_ID, MEAN, rainfall) %>%
                left_join(counties10,
                            by = "GEO_ID") %>%
                # create a column called "fips_code"
                mutate(fips_code = paste0(as.character(STATE),
                                         as.character(COUNTY)) %>% 
                               as.character()) %>%
                select()
        
        # save to file 
        rds_name <- dbf_file %>% 
                str_replace(pattern = dirname(.), 
                            replacement = "") %>%
                str_replace(pattern = ".dbf", replacement = ".rds")
        export(df, file = rds_name)
}
```


```{r}
list.files("zonal_stat",
           pattern = "^county-rainfall\\d{4}-\\d{2}-\\d{2}.dbf$",
           full.names = TRUE) %>% 
        map(~ clean_rainfall(.x))
```

Here is what the first few rows of *county-rainfall2006-11-01.rds* look like. 
```
> head(import('county-rainfall2006-11-01.rds'))
          GEO_ID     rainfall fips_code
1 0500000US01029 6.848943e-02     01029
2 0500000US01031 7.327194e-05     01031
3 0500000US01037 4.636359e-02     01037
4 0500000US01039 1.593135e-05     01039
5 0500000US01041 2.388091e-03     01041
6 0500000US01045 3.698660e-04     01045
```


# Visualizing the data 

I am going to create a gif animation showing the precipitation level from November 1st to November 7th, 2006. 

```{r}
library(rio)
library(tmap)
library(magrittr)
library(tidyverse)
library(sf)
```

In the next chunk, I am going to combine the seven rds files we created in the previous sections to one dataframe object called `full` while adding a date column. This combined dataframe will be joined to `us_counties` by the `GEO_ID` attribute.

*NB*: Notice that whereas in the previous section I read `gz_2010_us_050_00_500k` as spatial polygons dataframe using the `rgdal` package, here I am reading the same shapefiles as simple features object using the `sf` package. For more on geospatial data management, I would recommend reading Section 2 ["Geopgraphic Data in R"](https://geocompr.robinlovelace.net/spatial-class.html) in @lovelace2019geocomputation and [the `sf` package vignettes](https://r-spatial.github.io/sf/index.html).

The beauty of reading geospatial data as simple features object as opposed to SpatialPolygonsDataFrame is that I can access the geometry attribute as another column instead of another dataframe separate from that of non-geometric attributes. In creating the gif animation, I will be essentially grouping the combined data by the `date` attribute. And because the geometry feature is a polygon (i.e. more complex than points), it is necessary that I make my maps out of one dataframe object. 

```{r}
# read gz_2010_us_050_00_500k shapefiles as sf object
us_counties <- st_read("data/gz_2010_us_050_00_500k", "gz_2010_us_050_00_500k")

# combine county-rainfall<date>.rds files into one dataframe 
full <- list.files(full.names = TRUE,
           pattern = "^county-rainfall2006-\\d{2}-\\d{2}.rds$") %>%
        map_df(~{
                .x %>% import() %>% 
                        mutate(date = str_extract(.x, "2006-[0-9]{2}-[0-9]{2}")) 
                }) 

# join `full` to `us_counties`
us_counties %<>% left_join(full, by = "GEO_ID")
```

The final chunk is all `tmap`. 

First, I draw a boundary box that captures the coterminous US territory (`usa_main`). Next, I create an animated map using `tm_facets`, which splits and groups the dataframe `us_counties` by date. `tmap` uses ImageMagick to convert the faceted maps into a gif animation, and this might take some time and computer memory. The gif file can be played with `magick::image_read()`. 
```{r}
# set a boundary box
usa_main <- matrix(c(-125, 25, -66, 50), nrow = 2, byrow = FALSE)

# tmap animation
tmap_mode('plot')
facet_anim <- tm_basemap(server = "OpenStreetMap")  + 
        tm_shape(us_counties, bbox = usa_main) + 
        tm_fill("rainfall", colorNA = NULL) +
        tm_facets(along = "date", ncol = 1, nrow = 1, drop.NA.facets = TRUE,
                  drop.empty.facets = TRUE, showNA = FALSE) + 
        tm_layout(legend.position = c("right", "bottom"))
tmap_animation(tm = facet_anim, 
               filename = "fig/rainfall2006.gif", 
               delay = 50)
magick::image_read("fig/rainfall2006.gif")
```


![](/post/2019-05-04-collecting-historical-weather-data-part-iii_files/rainfall2006.gif)

Upon inspection, it turns out that I am missing one county in Vermont and one county in Washington for all seven days. It's possible that in 2006, these missing counties used to be part of an adjacent county but then branched off to become their own counties in 2010. I am not too concerned about this missing data because they are both remote parts of the United States. 

<hr> 

This completes the series on collecting historical weather data. There are so many tutorials on GIS data analysis and visualization, but this series, "Collecting historical weather data," begins from retrieving raw data from the NOAA server and performing interpolation on it, which I thought were rarely covered. I even had to rely heavily on @cooperman's supplementary material to the second part. I hope that this series will help others doing similar work. 

# References
