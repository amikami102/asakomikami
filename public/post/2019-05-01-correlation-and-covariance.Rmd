---
title: Correlation is just standardized covariance
date: '2019-05-01'
slug: correlation-is-just-standardized-covariance
excerpt: "Usually, the correlation of two random variables is introduced as the ratio of the covariance to the product of each variable's standard deiation. I share an alternative way of writing the correlation and cover three ways of writing the Cauchy-Schwartz Inequality encountered in probability."
categories: ['math']
tags: []
math: true
comment: true
bibliography: references.bib
output:
        blogdown::html_page:
                toc: false
---



Sometimes, we re-learn a concept in a new way that opens our eyes and feel like we finally *get* it. This happened to me with correlation: *the correlation of two random variables is just standardized covariance*!

*n.b.*: I learned the material in this article from taking Professor Daniel Weiner's course, *Mathematics of Statistics* at BU. All the credit of this post goes to him while all mistakes are mine. üíÅ

In this post, I want to show the math behind that statement and demonstrate how defining correlation this way makes it easier to understand the Cauchy-Schwartz Inequality. 

## Correlation as Standardized Covariance

According to my first textbook in probability, @ross2014first *A First Course in Probability*, the correlation of two random variables, $X$ and $Y$, denoted $\rho(X,Y)$ is defined as 
        $$\rho(X,Y) = \dfrac{\Cov(X,Y)}{\sqrt{\Var(X) \Var(Y)}}$$
given that $\Var(X)$ and $\Var(Y)$ exist. This definition can be more usefully stated as follows:
        $$ \rho(X,Y) = \Cov(X^{\rm{st}} Y^{\rm{st}})$$
where $X^{\rm{st}}$ and $Y^{\rm{st}}$ are rstandardized$X$ and $Y$ srespectively I will show that the two definitions are equivalent. 



**Proof**:
$$
\begin{align*}
        \rho(X, Y) & = \Cov(X^{\rm{st}} Y^{\rm{st}}) \\
                & = \E[X^{\rm{st}}Y^{\rm{st}}] - \E[X^{\rm{st}}]\E[Y^{\rm{st}}] \qquad \text{by definition of covariance} \\
                & = \E[X^{\rm{st}}Y^{\rm{st}}] \qquad \text{because $\E[X^{\rm{st}}] = \E[Y^{\rm{st}}] = 0$ by standardization} \\
                & = \E \bigg[ \bigg( \dfrac{X - \E[X]}{\sqrt{\Var(X)}} \bigg) \bigg( \dfrac{Y - \E[Y]}{\sqrt{\Var(Y)}} \bigg) \bigg] \\
                & = \dfrac{1}{\sqrt{\Var(X) \Var(Y)}} \E \big[ (X - \E[X])(Y - \E[Y]) \big] \\
                & = \dfrac{\Cov(X,Y)}{\sqrt{\Var(X) \Var(Y)}} \qquad \text{by definition of covariance} \qquad \heartsuit.
\end{align*}
$$
That's why correlation has no units. 

## Cauchy-Schwartz Inequality in Three Forms

@ross2014first mentions Cauchy-Schwartz Inequality (CS Inequality) in one of the exercises. The thoerem is stated like this:
        $$ \Big( \E[XY] \Big)^2 \leq \E[X^2]\E[Y^2].$$

**Proof**: We can prove this via the discriminant of quadratic formula. Let there be some constant $t$. Then, for random variables $X$ and $Y$, it holds that 
        \begin{align*}
        \E[(X + tY)^2] & = \E[X^2] + 2 \E[XY]t + \E[Y^2]t^2 \geq 0
        \end{align*}
because $\Var(X+tY) = \E[(X+ tY)^2] - \E[(X+tY)]^2 \geq 0$ if it exists. 
*n.b.*: Since $\E[(X+tY)]^2$ is positive, $\E[(X+tY)^2] = \Var(X+tY) + \E[(X+tY)]^2$ is a sum of two positive values and therefore must be non-negative. 

Then the discriminant of this quadratic equation with respect to $t$ must be equal to or less than 0, i.e. 
        $$ 
        \begin{align*}
        \big(2\E[XY]\big)^2 &- 4 \E[X^2]\E[Y^2] \leq 0 \\
        \big(\E[XY]\big)^2 &- \E[X^2]\E[Y^2]  \leq 0 \\
        \big(\E[XY] \big)^2 &\leq \E[X^2] \E[Y^2] \qquad \heartsuit.
        \end{align*}
        $$
That is one way of stating the CS Inequality. Another way of writing the CS Inequality is 
        $$ \Big( \Cov(X, Y) \Big)^2 \leq \Var(X)\Var(Y).$$
The proof of this form of CS Inequality follows the same steps as the first one except that the quadratic equation with respect to $t$ would be $ 0 \leq \Var(X + tY) = \Var(X) + 2\Cov(X, tY) + \Var(Y)t^2 = \Var(X) + 2t\Cov(X, Y) + \Var(Y) t^2$. We can see that the second form can be easily rewritten as the first form if we assume $\E[X] = \E[Y] = 0$, but the proof for the first form demonstrates that we don't need to assume $\E[X] = \E[Y] = 0$ to show the CS Inequality holds in that form. 

My favorite way of writing the CS Inequality, however, is with correlation:
        $$ \left| \rho(X,Y) \right| \leq 1.$$
We can see that this is an alternate way of writing the second form of CS Inequality, but even if we did not know the proof to the second form, we can prove this statement using the fact that a correlation is a covariance of standardized r.v.'s. 

**Proof**:
        $$
        \begin{align*}
        \left| \rho(X, Y) \right| & = \left| \Cov(X^{\rm{st}}Y^{\rm{st}}) \right| \\
        & = \left| \E \big[ X^{\rm{st}}Y^{\rm{st}} \big] \right| \\
        & \leq \E \Big[ \left| X^{\rm{st}}Y^{\rm{st}} \right| \Big] \qquad \text{by triangle inequality} \\
        & \leq \E \Big[ \dfrac{X^{\rm{st}} + Y^{\rm{st}}}{2} \Big] \qquad \text{by geometric inequality, $\left|X^{\rm{st}}Y^{\rm{st}}\right| \leq \dfrac{X^{\rm{st}} + Y^{\rm{st}}}{2}$} \\
        & = \dfrac{1}{2} \Big( \E\big[X^{\rm{st}}\big] + \E\big[Y^{\rm{st}}\big] \Big) \\
        & = \dfrac{1}{2}(1 + 1) = 1 \qquad \heartsuit.
        \end{align*}
        $$
Isn't that a neat proof? :satisfied:

## References