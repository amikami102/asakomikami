<!DOCTYPE html>
<html
  class=""
  lang="en-us"
  prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"
>
  <head>
    <meta charset="utf-8" />

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="HandheldFriendly" content="True" />
<meta name="MobileOptimized" content="320" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="generator" content="Hugo 0.67.1" />

<link rel="apple-touch-icon" sizes="180x180" href="//apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="//favicon-16x16.png">
<link rel="manifest" href="//site.webmanifest">
<link rel="mask-icon" href="//safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">



<meta name="keywords" content="math,
">

<meta property="og:title" content="Correlation is just standardized covariance" />
<meta property="og:description" content="Sometimes, we re-learn a concept in a new way that opens our eyes and feel like we finally get it. This happened to me with correlation: the correlation of two random variables is just standardized covariance!
n.b.: I learned the material in this article from taking Professor Daniel Weiner‚Äôs course, Mathematics of Statistics at BU. All the credit of this post goes to him while all mistakes are mine. üíÅ" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2019/05/01/correlation-is-just-standardized-covariance/" />
<meta property="article:published_time" content="2019-05-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-05-01T00:00:00+00:00" />

<meta property="og:site_name" content="Asako Mikami" />


<meta itemprop="name" content="Correlation is just standardized covariance">
<meta itemprop="description" content="Sometimes, we re-learn a concept in a new way that opens our eyes and feel like we finally get it. This happened to me with correlation: the correlation of two random variables is just standardized covariance!
n.b.: I learned the material in this article from taking Professor Daniel Weiner‚Äôs course, Mathematics of Statistics at BU. All the credit of this post goes to him while all mistakes are mine. üíÅ">
<meta itemprop="datePublished" content="2019-05-01T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-05-01T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="678">



<meta itemprop="keywords" content="math," />


    <title>Correlation is just standardized covariance</title>
    <link rel="canonical" href="../../../../2019/05/01/correlation-is-just-standardized-covariance/" />

    

    <link rel="stylesheet" href="../../../../css/reboot.css" />
<link rel="stylesheet" href="../../../../css/style.css" />
<script type="text/javascript" src="../../../../js/main.js"></script>


<link rel="stylesheet" href="../../../../highlight/styles/github-gist.css" rel="stylesheet" id="theme-stylesheet">
<script src="../../../../highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
</script>



<link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap" rel="stylesheet">
    <link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="../../../../apple-touch-icon.png" />
  </head>


<body
  class=" look-sheet-bkg"
  lang="en-us"
  itemscope
  itemtype="http://schema.org/Article">

  <div class="nav-bkg">
    <nav class="content-container pagewide-bar-padding">
      <span class="divider">/ </span>
      <a href="../../../../" >Asako Mikami</a><span class="divider">/ </span>
      <a href="../../../../post">Posts</a><ul class="list-unstyled right-links">

          <li>
            <a href="../../../../about/">
              <span class="post-title">About</span>
            </a>
          </li>

          <li>
            <a href="../../../../post/">
              <span class="post-title">All posts</span>
            </a>
          </li>

          <li>
            <a href="../../../../series/">
              <span class="post-title">Series</span>
            </a>
          </li>

</ul>

    </nav>
  </div>

  <section id="main" class="content-container look-sheet article-pad-v " itemprop="mainEntityOfPage">
    <h1 itemprop="name" id="title">Correlation is just standardized covariance</h1>
    <article itemprop="articleBody" id="content" class="article-body">
      


<p>Sometimes, we re-learn a concept in a new way that opens our eyes and feel like we finally <em>get</em> it. This happened to me with correlation: <em>the correlation of two random variables is just standardized covariance</em>!</p>
<p><em>n.b.</em>: I learned the material in this article from taking Professor Daniel Weiner‚Äôs course, <em>Mathematics of Statistics</em> at BU. All the credit of this post goes to him while all mistakes are mine. üíÅ</p>
<p>In this post, I want to show the math behind that statement and demonstrate how defining correlation this way makes it easier to understand the Cauchy-Schwartz Inequality.</p>
<div id="correlation-as-standardized-covariance" class="section level2">
<h2>Correlation as Standardized Covariance</h2>
<p>According to my first textbook in probability, <span class="citation">Ross (2014)</span> <em>A First Course in Probability</em>, the correlation of two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, denoted <span class="math inline">\(\rho(X,Y)\)</span> is defined as
<span class="math display">\[\rho(X,Y) = \dfrac{\Cov(X,Y)}{\sqrt{\Var(X) \Var(Y)}}\]</span>
given that <span class="math inline">\(\Var(X)\)</span> and <span class="math inline">\(\Var(Y)\)</span> exist. This definition can be more usefully stated as follows:
<span class="math display">\[ \rho(X,Y) = \Cov(X^{\rm{st}} Y^{\rm{st}})\]</span>
where <span class="math inline">\(X^{\rm{st}}\)</span> and <span class="math inline">\(Y^{\rm{st}}\)</span> are rstandardized<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> srespectively I will show that the two definitions are equivalent.</p>
<p><strong>Proof</strong>:
<span class="math display">\[
\begin{align*}
        \rho(X, Y) &amp; = \Cov(X^{\rm{st}} Y^{\rm{st}}) \\
                &amp; = \E[X^{\rm{st}}Y^{\rm{st}}] - \E[X^{\rm{st}}]\E[Y^{\rm{st}}] \qquad \text{by definition of covariance} \\
                &amp; = \E[X^{\rm{st}}Y^{\rm{st}}] \qquad \text{because $\E[X^{\rm{st}}] = \E[Y^{\rm{st}}] = 0$ by standardization} \\
                &amp; = \E \bigg[ \bigg( \dfrac{X - \E[X]}{\sqrt{\Var(X)}} \bigg) \bigg( \dfrac{Y - \E[Y]}{\sqrt{\Var(Y)}} \bigg) \bigg] \\
                &amp; = \dfrac{1}{\sqrt{\Var(X) \Var(Y)}} \E \big[ (X - \E[X])(Y - \E[Y]) \big] \\
                &amp; = \dfrac{\Cov(X,Y)}{\sqrt{\Var(X) \Var(Y)}} \qquad \text{by definition of covariance} \qquad \heartsuit.
\end{align*}
\]</span>
That‚Äôs why correlation has no units.</p>
</div>
<div id="cauchy-schwartz-inequality-in-three-forms" class="section level2">
<h2>Cauchy-Schwartz Inequality in Three Forms</h2>
<p><span class="citation">Ross (2014)</span> mentions Cauchy-Schwartz Inequality (CS Inequality) in one of the exercises. The thoerem is stated like this:
<span class="math display">\[ \Big( \E[XY] \Big)^2 \leq \E[X^2]\E[Y^2].\]</span></p>
<p><strong>Proof</strong>: We can prove this via the discriminant of quadratic formula. Let there be some constant <span class="math inline">\(t\)</span>. Then, for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, it holds that
<span class="math display">\[\begin{align*}
        \E[(X + tY)^2] &amp; = \E[X^2] + 2 \E[XY]t + \E[Y^2]t^2 \geq 0
        \end{align*}\]</span>
because <span class="math inline">\(\Var(X+tY) = \E[(X+ tY)^2] - \E[(X+tY)]^2 \geq 0\)</span> if it exists.
<em>n.b.</em>: Since <span class="math inline">\(\E[(X+tY)]^2\)</span> is positive, <span class="math inline">\(\E[(X+tY)^2] = \Var(X+tY) + \E[(X+tY)]^2\)</span> is a sum of two positive values and therefore must be non-negative.</p>
<p>Then the discriminant of this quadratic equation with respect to <span class="math inline">\(t\)</span> must be equal to or less than 0, i.e.
<span class="math display">\[ 
        \begin{align*}
        \big(2\E[XY]\big)^2 &amp;- 4 \E[X^2]\E[Y^2] \leq 0 \\
        \big(\E[XY]\big)^2 &amp;- \E[X^2]\E[Y^2]  \leq 0 \\
        \big(\E[XY] \big)^2 &amp;\leq \E[X^2] \E[Y^2] \qquad \heartsuit.
        \end{align*}
        \]</span>
That is one way of stating the CS Inequality. Another way of writing the CS Inequality is
<span class="math display">\[ \Big( \Cov(X, Y) \Big)^2 \leq \Var(X)\Var(Y).\]</span>
The proof of this form of CS Inequality follows the same steps as the first one except that the quadratic equation with respect to <span class="math inline">\(t\)</span> would be $ 0 (X + tY) = (X) + 2(X, tY) + (Y)t^2 = (X) + 2t(X, Y) + (Y) t^2$. We can see that the second form can be easily rewritten as the first form if we assume <span class="math inline">\(\E[X] = \E[Y] = 0\)</span>, but the proof for the first form demonstrates that we don‚Äôt need to assume <span class="math inline">\(\E[X] = \E[Y] = 0\)</span> to show the CS Inequality holds in that form.</p>
<p>My favorite way of writing the CS Inequality, however, is with correlation:
<span class="math display">\[ \left| \rho(X,Y) \right| \leq 1.\]</span>
We can see that this is an alternate way of writing the second form of CS Inequality, but even if we did not know the proof to the second form, we can prove this statement using the fact that a correlation is a covariance of standardized r.v.‚Äôs.</p>
<p><strong>Proof</strong>:
<span class="math display">\[
        \begin{align*}
        \left| \rho(X, Y) \right| &amp; = \left| \Cov(X^{\rm{st}}Y^{\rm{st}}) \right| \\
        &amp; = \left| \E \big[ X^{\rm{st}}Y^{\rm{st}} \big] \right| \\
        &amp; \leq \E \Big[ \left| X^{\rm{st}}Y^{\rm{st}} \right| \Big] \qquad \text{by triangle inequality} \\
        &amp; \leq \E \Big[ \dfrac{X^{\rm{st}} + Y^{\rm{st}}}{2} \Big] \qquad \text{by geometric inequality, $\left|X^{\rm{st}}Y^{\rm{st}}\right| \leq \dfrac{X^{\rm{st}} + Y^{\rm{st}}}{2}$} \\
        &amp; = \dfrac{1}{2} \Big( \E\big[X^{\rm{st}}\big] + \E\big[Y^{\rm{st}}\big] \Big) \\
        &amp; = \dfrac{1}{2}(1 + 1) = 1 \qquad \heartsuit.
        \end{align*}
        \]</span>
Isn‚Äôt that a neat proof? üòÜ</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-ross2014first">
<p>Ross, Sheldon. 2014. <em>A First Course in Probability</em>. Pearson.</p>
</div>
</div>
</div>

    </article>
  </section>
  

  </body>
</html>
