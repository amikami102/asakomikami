<!DOCTYPE html>
<html
  class=""
  lang="en-us"
  prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb#"
>
  <head>
    <meta charset="utf-8" />

    

  <ul>
    
      <li>
          <a href="../../../../tags/python/">python</a>
        </li>
      <li>
          <a href="../../../../tags/webscraping/">webscraping</a>
        </li></ul>


    <title>Collecting and parsing tweets, Part II</title>
    <link rel="canonical" href="../../../../2019/06/03/collecting-and-parsing-tweets-part2/" />

    

    <link rel="stylesheet" href="../../../../css/reboot.css" />
<link rel="stylesheet" href="../../../../css/style.css" />
<script type="text/javascript" src="../../../../js/main.js"></script>


<link rel="stylesheet" href="../../../../highlight/styles/github-gist.css" rel="stylesheet" id="theme-stylesheet">
<script src="../../../../highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
</script>



<link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=EB+Garamond&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=DM+Sans&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css2?family=Lato:wght@300;700;900&display=swap" rel="stylesheet">
    <link rel="shortcut icon" href="../../../../favicon.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="../../../../apple-touch-icon.png" />
  </head>


<body lang="en-us">
  <div class="nav-bkg">
    <nav class="content-container pagewide-bar-padding">
      <span class="divider">/ </span>
      <a href="../../../../" >Asako Mikami</a>
      <ul class="list-unstyled right-links">

          <li>
            <a href="../../../../about/">
              <span class="post-title">About</span>
            </a>
          </li>

          <li>
            <a href="../../../../post/">
              <span class="post-title">All posts</span>
            </a>
          </li>

          <li>
            <a href="../../../../series/">
              <span class="post-title">Series</span>
            </a>
          </li>

</ul>

    </nav>
  </div>

  <section id="main" class="content-container look-sheet article-pad-v " itemprop="mainEntityOfPage">
    <h1 itemprop="name" id="title">Collecting and parsing tweets, Part II</h1>

    <article itemprop="articleBody" id="content" class="article-body">
      

<div id="TOC">
<ul>
<li><a href="#clean-the-text"><span class="toc-section-number">1</span> Clean the text</a></li>
<li><a href="#find-address-in-the-text"><span class="toc-section-number">2</span> Find address in the text</a></li>
<li><a href="#if-__name__-__main__"><span class="toc-section-number">3</span> <code>if __name__ == &quot;__main__&quot;</code></a></li>
</ul>
</div>

<p>Part II will clean the texts and label whether the text contains any street address. Here’s a sketch of the data directory:</p>
<pre><code>|-- data/
     |-- raw/            # from Part I
     |-- 01_scraped/     # from Part I
     |-- 02_cleaned/     # output for Part II will go here
     |-- counts.json     # from Part I</code></pre>
<p>We will use the following packages and set up a logger.</p>
<pre class="python"><code>import json
import os
import sys
import requests
import logging
import re
import xml.etree.ElementTree as ET
from commonregex import CommonRegex
import time

# set up logger
log = logging.getLogger(__name__)
f_handler = logging.FileHandler(&#39;{}.log&#39;.format(__file__))
f_format = logging.basicConfig(format = &#39;%(asctime)s - %(message)s&#39;)
f_handler.setFormatter(f_format)
log.addHandler(f_handler)</code></pre>
<div id="clean-the-text" class="section level1">
<h1><span class="header-section-number">1</span> Clean the text</h1>
<p>First, we will clean the tweet text of links and special characters.</p>
<pre class="python"><code>def clean_tweet(tweet):
    &#39;&#39;&#39;
    Utility function to clean the text in a tweet by removing 
    links and special characters using regex.
    &#39;&#39;&#39;
    return &#39; &#39;.join(re.sub(&quot;(@[\S]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)&quot;, &quot; &quot;, tweet).split())</code></pre>
<p>The key here is the regular expression,</p>
<pre><code>(@[\S]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)</code></pre>
<p>which says to look for any portion of the text that are</p>
<ul>
<li>@ mentions, <code>(@[\S]+)</code>;</li>
<li>emojis, <code>([^0-9A-Za-z \t])</code>; or</li>
<li>url links, <code>(\w+:\/\/\S+)</code>.</li>
</ul>
<p>Along with this custom function, I am going to use a function from <a href="https://github.com/madisonmay/CommonRegex"><code>commonregex</code></a> called <code>CommonRegex()</code> to pick up street addresses, phone numbers, etc. in a string.</p>
<pre class="python"><code>regex_parser = CommonRegex()</code></pre>
</div>
<div id="find-address-in-the-text" class="section level1">
<h1><span class="header-section-number">2</span> Find address in the text</h1>
<p>The main function <code>find_address()</code> will use <code>clean_tweet()</code> and <code>regex_parser()</code> to clean the text and detect whether the text contains a street address. The function takes the input file, <em>scraped&lt;fromHour&gt;-&lt;toHour&gt;-&lt;page-count&gt;.json</em>, and does two things. For each dictionary in the file, the function adds two key-value items: <code>contain_address</code> whose value is logical; <code>clean_text</code> whose value is the cleaned text. It saves the value of <code>clean_text</code> into a new json file called <em>cleaned&lt;fromHour&gt;-&lt;toHour&gt;-&lt;page-count&gt;.json</em>.</p>
<pre class="python"><code>def find_address(jsonfile):
    &quot;&quot;&quot;
    Cleans tweet text.
    Adds entry {&#39;contain_address&#39;: (logical)} to tweet dictionary where 
    (logical) indicates whether the cleaned tweet contains street addresses.
    ===================
    jsonfile = (str of .json file name containing the tweets)
    &quot;&quot;&quot;
    # Load the JSON file 
    with open(os.path.join(args.script01dir, jsonfile), &quot;r&quot;, errors = &quot;ignore&quot;) as j:
        tweets = json.load(j)
        
    out_list = []
    c = 0 # counter for number of tweets containing address
    for tweet in tweets:
        query = clean_tweet(tweet[&#39;text&#39;])
        tweet.update({&#39;clean_text&#39;: query}) # add clean text to `tweet`
        # Check whether the cleaned text contains street address
        if regex_parser.street_addresses(query) == []:
            tweet.update({&#39;contain_address&#39;: False})
        else:
            if re.search(&#39;pizza&#39;, query) != None or re.search(&#39;1 8{1-3}[0-9]{1-2}&#39;, query) != None:
                tweet.update({&#39;contain_address&#39;: False})
                
            else:
                tweet.update({&#39;contain_address&#39;: True})
                c += 1
                out_list.append(tweet)
    
    # Save to &quot;cleaned*.json&quot; file
    cleanfile = jsonfile.replace(&quot;scraped&quot;, &quot;cleaned&quot;)
    with open(os.path.join(args.script02dir, cleanfile), &quot;w&quot;) as j:
        json.dump(out_list, j)
    
    # Log how many tweets were found containing address
    message = &quot;{} tweets containing address&quot;
    log.info(message.format(c))</code></pre>
</div>
<div id="if-__name__-__main__" class="section level1">
<h1><span class="header-section-number">3</span> <code>if __name__ == &quot;__main__&quot;</code></h1>
<pre class="python"><code>if __name__ == &quot;__main__&quot;:

    path = os.path.join(os.getcwd(), &#39;01_scraped&#39;)  
    for file in sorted(os.listdir(path)):
        find_address(file)
    
    
    print(&quot;Reached last file&quot;)
    sys.exit()</code></pre>
<p>Here is what <em>cleaned201811061300-201811061400-1.json</em> looks like.</p>
<pre><code>[{&quot;created_at&quot;: &quot;Tue Nov 06 13:08:08 +0000 2018&quot;, &quot;text&quot;: &quot;@PizzaToThePolls 3120 Washburn ave Minneapolis Mn https://t.co/cFGQINvuwW&quot;, &quot;id&quot;: 1059794562317729793, &quot;clean_text&quot;: &quot;3120 Washburn ave Minneapolis Mn&quot;, &quot;contain_address&quot;: true}]</code></pre>
<hr>
<p>Next in Part III, we will geocode these street addresses with Google’s Geocoding API. For how to save API keys in <code>.env</code> file, see this <a href="http://jonathansoma.com/lede/foundations-2019/classes/apis/keeping-api-keys-secret/">tutorial</a>.</p>
</div>

    </article>
  </section>
  

  </body>
</html>
